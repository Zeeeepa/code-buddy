# Chapitre 19 ‚Äî Infrastructure LLM Local üè†

---

## üé¨ Sc√®ne d'ouverture

*Un dimanche matin, dans l'appartement de Lina. Elle est assise sur son canap√©, laptop sur les genoux, une tasse de caf√© √† la main. √Ä l'√©cran, une facture AWS qui fait mal aux yeux.*

**Lina** *(marmonnant)* : "842 dollars. Ce mois-ci seulement. Pour des embeddings."

*Son t√©l√©phone vibre. Un message de Marc.*

**Marc** *(texto)* : "Tu as vu la facture OpenAI ? Karim va nous tuer."

*Lina soupire et regarde par la fen√™tre. Sa RTX 4070 ronronne doucement dans son PC gaming. 12 Go de VRAM qui ne servent qu'√† faire tourner Elden Ring le weekend.*

**Lina** *(√† elle-m√™me)* : "Et si..."

*Elle ouvre un terminal et tape :*

```bash
ollama run qwen2.5-7b
```

*Le mod√®le se charge. 4.5 Go. Il lui reste 7 Go de VRAM. Elle tape une question de code. La r√©ponse arrive en 2 secondes.*

**Lina** *(les yeux brillants)* : "C'est gratuit. C'est local. Et c'est rapide."

*Elle attrape son t√©l√©phone.*

**Lina** *(texto √† Marc)* : "RDV demain matin. J'ai une id√©e qui va changer Code Buddy."

---

## üìã Table des Mati√®res

| Section | Titre | Description |
|:-------:|-------|-------------|
| 19.1 | üéØ Le Calcul qui Fait Mal | Pourquoi aller local ? |
| 19.2 | üñ•Ô∏è GPU Monitor | Surveiller la VRAM en temps r√©el |
| 19.3 | üßÆ Ollama Embeddings | Embeddings locaux gratuits |
| 19.4 | üîç HNSW Vector Store | Recherche vectorielle pure TypeScript |
| 19.5 | üì¶ Model Hub | Gestion des mod√®les HuggingFace |
| 19.6 | üß† KV-Cache Configuration | Optimiser la m√©moire d'inf√©rence |
| 19.7 | ‚ö° Speculative Decoding | Acc√©l√©rer la g√©n√©ration 2-3x |
| 19.8 | üìä Benchmark Suite | Mesurer TTFT, TPS, p95 |
| 19.9 | üìù Schema Validator | Structured output fiable |

---

## 19.1 üéØ Le Calcul qui Fait Mal

### 19.1.1 üí∏ La R√©union du Lundi Matin

*Salle de r√©union. Lina, Marc et Karim sont assis autour de la table. Lina a pr√©par√© une pr√©sentation.*

**Karim** *(les bras crois√©s)* : "Alors, cette id√©e r√©volutionnaire ?"

**Lina** *(ouvrant son laptop)* : "Laisse-moi te montrer quelque chose."

*Elle affiche un tableur.*

**Lina** : "Voici nos co√ªts API du dernier mois compar√©s √† ce que √ßa pourrait √™tre avec une infrastructure locale."

![Comparaison des Co√ªts API](../images/cost-comparison.svg)

*Un silence s'installe.*

**Marc** : "Attends, tu veux dire qu'on peut faire tourner des embeddings gratuitement ?"

**Lina** : "Pas seulement les embeddings. La recherche vectorielle aussi. Et une bonne partie de l'inf√©rence pour les t√¢ches simples."

**Karim** *(se penchant en avant)* : "Explique-moi comment."

### 19.1.2 üéØ La Strat√©gie Hybride

**Lina** *(dessinant sur le whiteboard)* : "C'est une approche hybride. On utilise le local pour ce qui peut √™tre local, et le cloud pour ce qui n√©cessite vraiment de la puissance."

![Strat√©gie Hybride Local + Cloud](../images/hybrid-strategy.svg)

**Marc** : "Mais on a besoin de GPUs pour faire tourner √ßa, non ?"

**Lina** : "La plupart des d√©veloppeurs ont d√©j√† un GPU gaming. RTX 3060 avec 12 Go, RTX 4070 avec 12 Go... C'est amplement suffisant pour un mod√®le 7B quantifi√©."

**Karim** *(hochant la t√™te)* : "Je vois. Mais comment on g√®re la VRAM ? Les crashs ? Les mod√®les qui ne rentrent pas ?"

**Lina** *(souriant)* : "J'y ai pens√©. On va construire une vraie infrastructure. √âtape par √©tape."

---

## 19.2 üñ•Ô∏è GPU Monitor : La Vigie de la VRAM

### 19.2.1 üí• Le Probl√®me

*Le lendemain matin. Marc teste le prototype.*

**Marc** *(au t√©l√©phone avec Lina)* : "√áa a crash√©. 'CUDA out of memory'. J'ai essay√© de charger Devstral et √ßa a tout fait planter."

**Lina** : "Tu avais quoi d'autre en m√©moire GPU ?"

**Marc** : "Euh... VS Code, un navigateur avec WebGL, et peut-√™tre Discord avec l'overlay..."

**Lina** *(soupirant)* : "On a besoin d'un syst√®me qui v√©rifie la VRAM AVANT de charger un mod√®le. Pas apr√®s."

### 19.2.2 üîß La Solution : GPUMonitor

**Lina** *(quelques heures plus tard, sur Slack)* : "Marc, regarde ce que j'ai fait."

```typescript
// src/hardware/gpu-monitor.ts

import { EventEmitter } from 'events';

/**
 * üñ•Ô∏è GPUMonitor - La vigie de votre VRAM
 *
 * D√©tecte automatiquement :
 * - NVIDIA (nvidia-smi)
 * - AMD (rocm-smi)
 * - Apple Silicon (system_profiler)
 * - Intel Arc (intel_gpu_top)
 */
export class GPUMonitor extends EventEmitter {
  private vendor: 'nvidia' | 'amd' | 'apple' | 'intel' | 'unknown';
  private totalVRAM: number = 0;
  private refreshInterval?: NodeJS.Timer;

  async initialize(): Promise<void> {
    // D√©tection automatique du GPU
    this.vendor = await this.detectVendor();
    this.totalVRAM = await this.getTotalVRAM();

    console.log(`üñ•Ô∏è [GPUMonitor] Detected: ${this.vendor} (${this.totalVRAM} MB)`);
  }

  async getStats(): Promise<GPUStats> {
    const used = await this.getUsedVRAM();
    const free = this.totalVRAM - used;
    const percent = (used / this.totalVRAM) * 100;

    // Alertes automatiques
    if (percent > 90) {
      this.emit('critical', { used, free, percent });
    } else if (percent > 75) {
      this.emit('warning', { used, free, percent });
    }

    return {
      totalVRAM: this.totalVRAM,
      usedVRAM: used,
      freeVRAM: free,
      usagePercent: percent,
      vendor: this.vendor,
    };
  }

  /**
   * üßÆ Recommandation pour charger un mod√®le
   */
  calculateOffloadRecommendation(modelSizeMB: number): OffloadRecommendation {
    const stats = this.getStatsSync();
    const safetyMargin = 500; // Garder 500 MB de marge
    const available = stats.freeVRAM - safetyMargin;

    if (modelSizeMB <= available) {
      return {
        shouldOffload: false,
        suggestedGpuLayers: -1, // Toutes les layers
        reason: 'Model fits entirely in VRAM',
      };
    }

    // Calcul des layers √† garder en GPU
    const ratio = available / modelSizeMB;
    const suggestedLayers = Math.floor(ratio * 32); // Assume 32 layers

    return {
      shouldOffload: true,
      suggestedGpuLayers: Math.max(0, suggestedLayers),
      reason: `Model (${modelSizeMB} MB) exceeds available VRAM (${available} MB)`,
    };
  }
}
```

**Marc** *(testant)* : "Ok, √ßa marche. Mais comment √ßa d√©tecte le GPU ?"

![GPUMonitor Architecture](../images/gpu-monitor-architecture.svg)

**Lina** : "Multi-vendor. Regarde :"

```typescript
private async detectVendor(): Promise<string> {
  // NVIDIA - nvidia-smi
  try {
    const result = await execAsync('nvidia-smi --query-gpu=name --format=csv,noheader');
    if (result.stdout.trim()) return 'nvidia';
  } catch {}

  // AMD - rocm-smi
  try {
    const result = await execAsync('rocm-smi --showproductname');
    if (result.stdout.includes('GPU')) return 'amd';
  } catch {}

  // Apple Silicon - system_profiler
  try {
    const result = await execAsync('system_profiler SPDisplaysDataType');
    if (result.stdout.includes('Apple')) return 'apple';
  } catch {}

  // Intel Arc
  try {
    const result = await execAsync('intel_gpu_top -l 1');
    if (result.stdout) return 'intel';
  } catch {}

  return 'unknown';
}
```

**Marc** : "G√©nial. Et si quelqu'un n'a pas de GPU ?"

**Lina** : "On tombe sur le CPU avec un warning. Mais franchement, en 2024, qui n'a pas un GPU ?"

**Marc** : "Les laptops d'entreprise."

**Lina** *(grima√ßant)* : "Touch√©."

---

## 19.3 üßÆ Ollama Embeddings : Le Graal des Embeddings Gratuits

### 19.3.1 üí° L'Illumination

*Pause caf√©. Lina montre son √©cran √† Marc.*

**Lina** : "Tu sais combien on paie pour embeddings ?"

**Marc** : "Environ 300 dollars par mois, non ?"

**Lina** : "Oui. Pour convertir du texte en vecteurs. Une op√©ration que n'importe quel mod√®le local peut faire."

*Elle ouvre un terminal.*

```bash
# Installation Ollama (si pas d√©j√† fait)
curl -fsSL https://ollama.ai/install.sh | sh

# T√©l√©charger un mod√®le d'embedding
ollama pull nomic-embed-text

# Test
curl http://localhost:11434/api/embeddings \
  -d '{"model": "nomic-embed-text", "prompt": "Hello world"}'
```

**Marc** *(regardant la r√©ponse)* : "768 dimensions. C'est pareil qu'ada-002."

**Lina** : "Exactement. Mais gratuit. Et local. Et sans latence r√©seau."

### 19.3.2 üèóÔ∏è L'Int√©gration

```typescript
// src/context/codebase-rag/ollama-embeddings.ts

/**
 * üßÆ OllamaEmbeddingProvider - Embeddings locaux via Ollama
 *
 * Mod√®les recommand√©s :
 * - nomic-embed-text : 768 dims, excellent rapport qualit√©/vitesse
 * - mxbai-embed-large : 1024 dims, meilleure qualit√©
 * - all-minilm : 384 dims, ultra-rapide
 */
export class OllamaEmbeddingProvider extends EventEmitter {
  private baseUrl: string;
  private model: string;
  private dimensions: number;

  constructor(options: OllamaEmbeddingOptions = {}) {
    super();
    this.baseUrl = options.baseUrl ?? 'http://localhost:11434';
    this.model = options.model ?? 'nomic-embed-text';
    this.dimensions = options.dimensions ?? 768;
  }

  async embed(text: string): Promise<number[]> {
    const response = await fetch(`${this.baseUrl}/api/embeddings`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.model,
        prompt: text,
      }),
    });

    const data = await response.json();
    return data.embedding;
  }

  async embedBatch(texts: string[]): Promise<number[][]> {
    const results: number[][] = [];

    for (let i = 0; i < texts.length; i++) {
      const embedding = await this.embed(texts[i]);
      results.push(embedding);

      // √âv√©nement de progression
      this.emit('batch:progress', i + 1, texts.length);
    }

    return results;
  }

  /**
   * üìè Similarit√© cosinus entre deux embeddings
   */
  similarity(a: number[], b: number[]): number {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;

    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }

    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }
}
```

**Marc** : "Et la qualit√© ? C'est aussi bon qu'OpenAI ?"

**Lina** : "Pour du code ? Pratiquement identique. Les benchmarks MTEB montrent que nomic-embed-text est dans le top 10 pour les t√¢ches techniques."

**Marc** : "Et la vitesse ?"

**Lina** : "Plus rapide que le cloud. Pas de latence r√©seau. Sur mon RTX 4070, je fais 200 embeddings par seconde."

---

## 19.4 üîç HNSW Vector Store : Recherche Sans D√©pendances

### 19.4.1 ü§î Le Dilemme des Vector Databases

*R√©union de design. Lina pr√©sente les options.*

**Lina** : "Ok, maintenant qu'on a les embeddings, il nous faut un endroit pour les stocker et les chercher."

**Karim** : "Pinecone ? Weaviate ?"

**Lina** : "Cloud. √áa nous co√ªte 89 dollars par mois et √ßa ajoute 50ms de latence."

**Marc** : "FAISS ?"

**Lina** : "Python. On devrait maintenir des bindings natifs."

**Karim** : "ChromaDB ?"

**Lina** : "Serveur s√©par√© √† g√©rer. Plus de complexit√© op√©rationnelle."

*Elle marque une pause.*

**Lina** : "J'ai une autre id√©e. On l'√©crit nous-m√™mes."

**Karim** *(levant un sourcil)* : "Tu veux √©crire une base de donn√©es vectorielle ?"

**Lina** : "Pas une base compl√®te. Juste l'algorithme de recherche. HNSW. En TypeScript pur."

### 19.4.2 üß† L'Algorithme HNSW Expliqu√©

**Marc** : "HNSW ? C'est quoi ?"

**Lina** *(au whiteboard)* : "Hierarchical Navigable Small World. C'est un graphe multi-niveaux pour la recherche de plus proches voisins."

![HNSW Structure](../images/hnsw-structure.svg)

**Marc** : "Donc au lieu de comparer avec tous les vecteurs..."

**Lina** : "On navigue dans le graphe. Comme chercher dans un annuaire t√©l√©phonique au lieu de lire toutes les pages."

### 19.4.3 üîß L'Impl√©mentation

```typescript
// src/context/codebase-rag/hnsw-store.ts

/**
 * üîç HNSWVectorStore - Recherche vectorielle pure TypeScript
 *
 * Avantages :
 * - Z√©ro d√©pendance externe
 * - Persistance JSON (portable)
 * - O(log N) pour la recherche
 */
export class HNSWVectorStore extends EventEmitter {
  private nodes: Map<string, HNSWNode> = new Map();
  private levels: Map<string, number> = new Map();
  private maxLevel: number = 0;
  private entryPoint: string | null = null;

  // Param√®tres de l'algorithme
  private M: number;              // Connexions max par noeud
  private efConstruction: number; // Qualit√© de construction
  private dimensions: number;

  constructor(options: HNSWOptions) {
    super();
    this.dimensions = options.dimensions;
    this.M = options.M ?? 16;
    this.efConstruction = options.efConstruction ?? 200;
  }

  /**
   * ‚ûï Ajouter un vecteur
   */
  add(item: VectorItem): void {
    const { id, vector, metadata } = item;

    // V√©rification des dimensions
    if (vector.length !== this.dimensions) {
      throw new Error(`Dimension mismatch: expected ${this.dimensions}, got ${vector.length}`);
    }

    // Niveau al√©atoire (distribution exponentielle)
    const level = this.getRandomLevel();

    // Cr√©er le noeud
    const node: HNSWNode = {
      id,
      vector,
      metadata,
      connections: new Map(), // niveau ‚Üí Set<voisins>
    };

    // Initialiser les connexions pour chaque niveau
    for (let l = 0; l <= level; l++) {
      node.connections.set(l, new Set());
    }

    this.nodes.set(id, node);
    this.levels.set(id, level);

    // Si c'est le premier noeud
    if (!this.entryPoint) {
      this.entryPoint = id;
      this.maxLevel = level;
      return;
    }

    // Connecter aux voisins existants
    this.connectNode(id, level);

    // Mettre √† jour le point d'entr√©e si n√©cessaire
    if (level > this.maxLevel) {
      this.entryPoint = id;
      this.maxLevel = level;
    }

    this.emit('add', { id, level });
  }

  /**
   * üîç Recherche des k plus proches voisins
   */
  search(query: number[], k: number = 5): SearchResult[] {
    if (!this.entryPoint) return [];

    let currentNode = this.entryPoint;

    // Descendre les niveaux sup√©rieurs
    for (let level = this.maxLevel; level > 0; level--) {
      currentNode = this.greedySearch(query, currentNode, level);
    }

    // Recherche exhaustive au niveau 0
    const candidates = this.searchLevel(query, currentNode, 0, Math.max(k, this.efConstruction));

    // Trier et retourner les k meilleurs
    return candidates
      .sort((a, b) => a.distance - b.distance)
      .slice(0, k);
  }

  /**
   * üíæ Sauvegarder l'index
   */
  async save(path: string): Promise<void> {
    const data = {
      dimensions: this.dimensions,
      M: this.M,
      efConstruction: this.efConstruction,
      maxLevel: this.maxLevel,
      entryPoint: this.entryPoint,
      nodes: Object.fromEntries(
        Array.from(this.nodes.entries()).map(([id, node]) => [
          id,
          {
            ...node,
            connections: Object.fromEntries(
              Array.from(node.connections.entries()).map(([l, s]) => [l, [...s]])
            ),
          },
        ])
      ),
      levels: Object.fromEntries(this.levels),
    };

    await fs.writeFile(path, JSON.stringify(data, null, 2));
    this.emit('save', { path, nodeCount: this.nodes.size });
  }
}
```

**Karim** : "Quelle est la performance compar√©e √† FAISS ?"

**Lina** : "FAISS est l√©g√®rement plus rapide ‚Äî environ 0.5ms contre 1ms pour notre impl√©mentation. Mais on √©limine toute la complexit√© des bindings Python."

**Marc** : "Et pour 100,000 vecteurs ?"

**Lina** : "Toujours sous 5ms. HNSW scale logarithmiquement."

---

## 19.5 üì¶ Model Hub : Le Gestionnaire de Mod√®les

### 19.5.1 üò§ Le Cauchemar du T√©l√©chargement Manuel

*Marc essaie de t√©l√©charger un mod√®le.*

**Marc** *(frustr√©)* : "C'est quoi tous ces fichiers sur HuggingFace ? Q4_K_M, Q5_K_S, Q6_K, Q8_0... Lequel je prends ?"

**Lina** : "√áa d√©pend de ta VRAM."

**Marc** : "J'ai 8 Go."

**Lina** : "Alors Q4_K_M pour un 7B."

**Marc** : "Et je le mets o√π ?"

**Lina** : "Dans un dossier standard. Mais attends..."

*Elle r√©fl√©chit.*

**Lina** : "On devrait automatiser tout √ßa."

### 19.5.2 üõ†Ô∏è Le Model Hub

```typescript
// src/models/model-hub.ts

/**
 * üì¶ ModelHub - Gestionnaire de mod√®les HuggingFace
 *
 * Fonctionnalit√©s :
 * - T√©l√©chargement avec progression
 * - S√©lection automatique de quantization
 * - Estimation VRAM
 * - Organisation standardis√©e
 */
export class ModelHub extends EventEmitter {
  private modelsDir: string;
  private downloadedModels: Map<string, DownloadedModel> = new Map();

  constructor(options: ModelHubOptions = {}) {
    super();
    this.modelsDir = options.modelsDir ?? path.join(os.homedir(), '.codebuddy', 'models');
  }

  /**
   * üì• T√©l√©charger un mod√®le
   */
  async download(
    modelId: string,
    quantization: QuantizationType = 'Q4_K_M'
  ): Promise<string> {
    const modelInfo = RECOMMENDED_MODELS[modelId];
    if (!modelInfo) {
      throw new Error(`Unknown model: ${modelId}`);
    }

    const filename = `${modelId}.${quantization}.gguf`;
    const targetPath = path.join(this.modelsDir, filename);

    // V√©rifier si d√©j√† t√©l√©charg√©
    if (await this.fileExists(targetPath)) {
      this.emit('download:cached', { modelId, path: targetPath });
      return targetPath;
    }

    // URL HuggingFace
    const url = this.buildDownloadUrl(modelInfo.repo, filename);

    // T√©l√©chargement avec progression
    const response = await fetch(url);
    const totalBytes = parseInt(response.headers.get('content-length') || '0');
    let downloadedBytes = 0;

    const reader = response.body!.getReader();
    const chunks: Uint8Array[] = [];

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      chunks.push(value);
      downloadedBytes += value.length;

      const percent = (downloadedBytes / totalBytes) * 100;
      const speedMBps = downloadedBytes / 1024 / 1024; // Simplified

      this.emit('download:progress', {
        modelId,
        percent: percent.toFixed(1),
        downloadedMB: (downloadedBytes / 1024 / 1024).toFixed(1),
        totalMB: (totalBytes / 1024 / 1024).toFixed(1),
        speedMBps: speedMBps.toFixed(1),
      });
    }

    // √âcrire le fichier
    const buffer = Buffer.concat(chunks);
    await fs.writeFile(targetPath, buffer);

    this.emit('download:complete', { modelId, path: targetPath });
    return targetPath;
  }

  /**
   * üßÆ Estimer la VRAM n√©cessaire
   */
  estimateVRAM(modelId: string, quantization: QuantizationType): number {
    const model = RECOMMENDED_MODELS[modelId];
    if (!model) return 0;

    const paramsBillions = model.params / 1_000_000_000;
    const bitsPerWeight = QUANTIZATION_BITS[quantization];

    // Formule : params √ó bits / 8 √ó 1.2 (overhead)
    return Math.ceil(paramsBillions * bitsPerWeight / 8 * 1.2 * 1024);
  }
}

/**
 * üìã Mod√®les recommand√©s pour le code
 */
export const RECOMMENDED_MODELS = {
  'devstral-7b': {
    params: 7_000_000_000,
    repo: 'mistralai/devstral-7B-v0.1-GGUF',
    description: 'Code generation, Mistral-based',
  },
  'qwen2.5-7b': {
    params: 7_000_000_000,
    repo: 'Qwen/Qwen2.5-7B-Instruct-GGUF',
    description: 'General + code, excellent quality',
  },
  'llama-3.2-3b': {
    params: 3_000_000_000,
    repo: 'meta-llama/Llama-3.2-3B-Instruct-GGUF',
    description: 'Compact, fast, general purpose',
  },
  'deepseek-coder-6.7b': {
    params: 6_700_000_000,
    repo: 'deepseek-ai/deepseek-coder-6.7b-instruct-GGUF',
    description: 'Code specialized, instruction-tuned',
  },
};
```

**Marc** : "Et les quantizations, c'est quoi exactement ?"

**Lina** : "C'est la pr√©cision des poids du mod√®le. Moins de bits = fichier plus petit, mais qualit√© potentiellement r√©duite."

![Guide des Quantizations GGUF](../images/quantization-guide.svg)

**Marc** : "Et tout √ßa s'assemble comment ?"

**Lina** : "Voici le pipeline complet :"

![RAG Pipeline Local](../images/rag-pipeline-local.svg)

---

## 19.6 üß† KV-Cache Configuration : Le Secret de la M√©moire

### 19.6.1 üí° La D√©couverte

*Une semaine plus tard. Lina lit un article technique.*

**Lina** *(excit√©e, appelant Marc)* : "Marc ! Tu savais que le KV-cache peut prendre jusqu'√† 30% de la VRAM ?"

**Marc** : "Le quoi ?"

**Lina** : "Le cache des cl√©s et valeurs de l'attention. C'est ce qui permet au mod√®le de se 'souvenir' du contexte."

**Marc** : "Et on peut l'optimiser ?"

**Lina** : "Oui ! On peut le quantifier comme les poids du mod√®le. q8_0 au lieu de f16 = 50% de m√©moire en moins."

### 19.6.2 üîß Le KVCacheManager

```typescript
// src/inference/kv-cache-config.ts

/**
 * üß† KVCacheManager - Optimisation de la m√©moire d'inf√©rence
 *
 * Le KV-Cache stocke les cl√©s et valeurs de l'attention.
 * Plus le contexte est long, plus il prend de m√©moire.
 *
 * Formule : 2 √ó n_layers √ó context_length √ó n_kv_heads √ó head_dim √ó bytes
 */
export class KVCacheManager extends EventEmitter {
  private config: KVCacheConfig;
  private architecture?: ModelArchitecture;

  constructor(config: Partial<KVCacheConfig> = {}) {
    super();
    this.config = {
      contextLength: config.contextLength ?? 4096,
      kvQuantization: config.kvQuantization ?? 'f16',
      flashAttention: config.flashAttention ?? true,
      batchSize: config.batchSize ?? 512,
      ...config,
    };
  }

  /**
   * üîç D√©finir l'architecture du mod√®le
   */
  setArchitecture(model: string | ModelArchitecture): void {
    if (typeof model === 'string') {
      // D√©tection automatique par nom
      this.architecture = this.detectArchitecture(model);
    } else {
      this.architecture = model;
    }
  }

  /**
   * üìä Estimer la m√©moire du KV-Cache
   */
  estimateMemory(contextLength?: number, batchSize: number = 1): MemoryEstimate {
    const ctx = contextLength ?? this.config.contextLength;
    const arch = this.architecture ?? DEFAULT_ARCHITECTURE;

    // Calcul de la taille
    const headDim = arch.nEmbed / arch.nHead;
    const kvHeads = arch.nKVHead ?? arch.nHead;

    // Taille par layer : 2 (K+V) √ó ctx √ó kvHeads √ó headDim √ó batch
    const bytesPerElem = QUANTIZATION_BYTES[this.config.kvQuantization];
    const perLayerBytes = 2 * ctx * kvHeads * headDim * batchSize * bytesPerElem;
    const totalBytes = perLayerBytes * arch.nLayers;

    const gpuMemoryMB = totalBytes / (1024 * 1024);

    // Recommandation
    let recommendation = 'Configuration optimale';
    if (gpuMemoryMB > 4000) {
      recommendation = 'Consid√©rez kvQuantization: q4_0 ou r√©duisez contextLength';
    } else if (gpuMemoryMB > 2000) {
      recommendation = 'Consid√©rez kvQuantization: q8_0 pour √©conomiser de la m√©moire';
    }

    return {
      totalBytes,
      perLayerBytes,
      gpuMemoryMB,
      fitsInVRAM: gpuMemoryMB < 6000, // Assume 8GB GPU with margin
      recommendation,
    };
  }

  /**
   * üõ†Ô∏è G√©n√©rer les arguments llama.cpp
   */
  generateLlamaCppArgs(): string[] {
    const args: string[] = [];

    // Context length
    args.push('-c', this.config.contextLength.toString());

    // Batch sizes
    if (this.config.batchSize) {
      args.push('-b', this.config.batchSize.toString());
    }

    // KV quantization (si pas f16)
    if (this.config.kvQuantization !== 'f16') {
      args.push('--cache-type-k', this.config.kvQuantization);
      args.push('--cache-type-v', this.config.kvQuantization);
    }

    // Flash attention
    if (this.config.flashAttention) {
      args.push('-fa');
    }

    // GPU layers
    if (this.architecture && this.config.cpuOffloadLayers) {
      const gpuLayers = this.architecture.nLayers - this.config.cpuOffloadLayers;
      args.push('-ngl', gpuLayers.toString());
    }

    return args;
  }
}

/**
 * üìã Architectures de mod√®les connues
 */
export const MODEL_ARCHITECTURES: Record<string, ModelArchitecture> = {
  'qwen2.5-7b': { nLayers: 28, nEmbed: 3584, nHead: 28, nKVHead: 4 },
  'qwen2.5-14b': { nLayers: 40, nEmbed: 5120, nHead: 40, nKVHead: 8 },
  'llama-3.1-8b': { nLayers: 32, nEmbed: 4096, nHead: 32, nKVHead: 8 },
  'devstral-7b': { nLayers: 32, nEmbed: 4096, nHead: 32, nKVHead: 8 },
  'deepseek-coder-6.7b': { nLayers: 32, nEmbed: 4096, nHead: 32, nKVHead: 32 },
};
```

![KV-Cache et Quantization](../images/kv-cache-quantization.svg)

**Marc** : "Donc avec q8_0 au lieu de f16, je peux doubler mon contexte ?"

**Lina** : "Exactement. Ou garder le m√™me contexte avec beaucoup plus de marge pour le mod√®le lui-m√™me."

---

## 19.7 ‚ö° Speculative Decoding : L'Acc√©l√©rateur de G√©n√©ration

### 19.7.1 üê¢ Le Probl√®me de la G√©n√©ration Lente

*Marc regarde un benchmark.*

**Marc** : "30 tokens par seconde. C'est pas terrible pour du code."

**Lina** : "C'est parce que la g√©n√©ration auto-r√©gressive est lente par nature. Un token = un forward pass complet."

**Marc** : "Il n'y a pas moyen d'acc√©l√©rer ?"

**Lina** *(souriant myst√©rieusement)* : "Il y a une technique. Speculative Decoding."

### 19.7.2 üí° L'Id√©e G√©niale

**Lina** *(au whiteboard)* : "L'id√©e est simple mais brillante. On utilise un petit mod√®le rapide pour 'deviner' plusieurs tokens, puis on v√©rifie avec le grand mod√®le."

![Speculative Decoding](../images/speculative-decoding.svg)

**Marc** : "Attends, le grand mod√®le peut v√©rifier plusieurs tokens en un seul pass ?"

**Lina** : "Exactement ! C'est parce que l'attention peut traiter toute la s√©quence en parall√®le. Le co√ªt marginal d'un token suppl√©mentaire est tr√®s faible."

### 19.7.3 üîß L'Impl√©mentation

```typescript
// src/inference/speculative-decoding.ts

/**
 * ‚ö° SpeculativeDecoder - Acc√©l√©ration de g√©n√©ration 2-3x
 *
 * Principe :
 * 1. Le mod√®le draft g√©n√®re K tokens rapidement
 * 2. Le mod√®le target v√©rifie ces tokens en un pass
 * 3. Les tokens accept√©s sont gard√©s, sinon on recommence
 */
export class SpeculativeDecoder extends EventEmitter {
  private config: SpeculativeConfig;
  private stats: SpeculativeStats;

  constructor(config: Partial<SpeculativeConfig> = {}) {
    super();
    this.config = {
      draftModel: config.draftModel ?? 'qwen2.5-0.5b',
      targetModel: config.targetModel ?? 'qwen2.5-7b',
      speculationLength: config.speculationLength ?? 4,
      acceptanceThreshold: config.acceptanceThreshold ?? 0.8,
      adaptive: config.adaptive ?? true,
    };

    this.stats = {
      totalTokens: 0,
      acceptedTokens: 0,
      rejectedTokens: 0,
      draftTime: 0,
      verifyTime: 0,
    };
  }

  /**
   * üöÄ G√©n√©ration acc√©l√©r√©e
   */
  async generate(
    prompt: string,
    draftCallback: DraftCallback,
    targetCallback: TargetCallback,
    options: GenerateOptions = {}
  ): Promise<GenerateResult> {
    const maxTokens = options.maxTokens ?? 256;
    let generated: string[] = [];

    while (generated.length < maxTokens) {
      // 1Ô∏è‚É£ Draft : g√©n√©rer K tokens candidats
      const draftStart = Date.now();
      const draftTokens = await draftCallback(
        prompt + generated.join(''),
        this.config.speculationLength
      );
      this.stats.draftTime += Date.now() - draftStart;

      // 2Ô∏è‚É£ Verify : v√©rifier avec le grand mod√®le
      const verifyStart = Date.now();
      const verification = await targetCallback(
        prompt + generated.join(''),
        draftTokens
      );
      this.stats.verifyTime += Date.now() - verifyStart;

      // 3Ô∏è‚É£ Accept/Reject
      const accepted = draftTokens.slice(0, verification.acceptedCount);
      generated.push(...accepted);

      this.stats.totalTokens += draftTokens.length;
      this.stats.acceptedTokens += accepted.length;
      this.stats.rejectedTokens += draftTokens.length - accepted.length;

      // √âmettre √©v√©nement
      this.emit('generation', {
        accepted: accepted.length,
        rejected: draftTokens.length - accepted.length,
        total: generated.length,
      });

      // Adaptation de la longueur de sp√©culation
      if (this.config.adaptive) {
        this.adaptSpeculationLength();
      }

      // Si aucun token accept√©, on a fini
      if (accepted.length === 0) break;
    }

    return {
      tokens: generated,
      content: generated.join(''),
      stats: this.getStats(),
    };
  }

  /**
   * üìä Statistiques de performance
   */
  getStats(): SpeculativeStats & { acceptanceRate: number; speedup: number } {
    const acceptanceRate = this.stats.totalTokens > 0
      ? this.stats.acceptedTokens / this.stats.totalTokens
      : 0;

    // Speedup estim√© : tokens accept√©s / (1 + overhead draft)
    const draftOverhead = this.stats.draftTime / (this.stats.draftTime + this.stats.verifyTime);
    const speedup = acceptanceRate * this.config.speculationLength / (1 + draftOverhead);

    return {
      ...this.stats,
      acceptanceRate,
      speedup,
    };
  }

  /**
   * üéöÔ∏è Adaptation automatique
   */
  private adaptSpeculationLength(): void {
    const rate = this.stats.acceptedTokens / this.stats.totalTokens;

    const oldLength = this.config.speculationLength;

    if (rate > 0.9 && this.config.speculationLength < 8) {
      this.config.speculationLength++;
    } else if (rate < 0.5 && this.config.speculationLength > 2) {
      this.config.speculationLength--;
    }

    if (oldLength !== this.config.speculationLength) {
      this.emit('adaptiveAdjust', {
        oldLength,
        newLength: this.config.speculationLength,
        reason: rate > 0.9 ? 'high_acceptance' : 'low_acceptance',
      });
    }
  }
}

/**
 * üìã Paires draft/target recommand√©es
 */
export const RECOMMENDED_PAIRS = {
  'qwen2.5-7b': { draft: 'qwen2.5-0.5b', speedup: '2-3x' },
  'qwen2.5-14b': { draft: 'qwen2.5-1.5b', speedup: '2-2.5x' },
  'llama-3.1-8b': { draft: 'llama-3.2-1b', speedup: '2-3x' },
};
```

**Marc** : "Et √ßa marche vraiment ?"

**Lina** : "Oui ! En production, on voit des speedups de 2-3x sur du code. Le taux d'acceptation est souvent au-dessus de 70%."

---

## 19.8 üìä Benchmark Suite : Mesurer Pour Optimiser

### 19.8.1 üìà L'Importance des M√©triques

*R√©union d'√©quipe. Karim pose LA question.*

**Karim** : "Comment on sait si notre infrastructure locale est vraiment meilleure ?"

**Lina** : "On mesure. Rigoureusement."

**Marc** : "Mesurer quoi exactement ?"

**Lina** : "TTFT, TPS, p95. Les trois m√©triques qui comptent."

![Les 3 M√©triques Essentielles](../images/metrics-essentielles.svg)

### 19.8.2 üõ†Ô∏è La BenchmarkSuite

```typescript
// src/performance/benchmark-suite.ts

/**
 * üìä BenchmarkSuite - Mesure rigoureuse des performances LLM
 *
 * M√©triques collect√©es :
 * - Latence (moyenne, min, max, stdDev)
 * - TTFT avec percentiles (p50, p95, p99)
 * - TPS (tokens par seconde)
 * - Throughput (requ√™tes par seconde)
 * - Co√ªt estim√©
 */
export class BenchmarkSuite extends EventEmitter {
  private config: BenchmarkConfig;

  constructor(config: Partial<BenchmarkConfig> = {}) {
    super();
    this.config = {
      warmupRuns: config.warmupRuns ?? 2,
      runs: config.runs ?? 10,
      concurrency: config.concurrency ?? 1,
      timeout: config.timeout ?? 60000,
      prompts: config.prompts ?? DEFAULT_PROMPTS,
    };
  }

  /**
   * üèÉ Ex√©cuter le benchmark
   */
  async run(
    modelName: string,
    callback: BenchmarkCallback
  ): Promise<BenchmarkResults> {
    const runs: RunResult[] = [];
    const prompts = this.config.prompts;

    // Phase de warmup
    this.emit('phase', { phase: 'warmup', total: this.config.warmupRuns });
    for (let i = 0; i < this.config.warmupRuns; i++) {
      const prompt = prompts[i % prompts.length];
      await this.executeRun(prompt.prompt, callback, true);
    }

    // Phase de benchmark
    this.emit('phase', { phase: 'benchmark', total: this.config.runs });
    for (let i = 0; i < this.config.runs; i++) {
      const prompt = prompts[i % prompts.length];
      const result = await this.executeRun(prompt.prompt, callback, false);
      runs.push(result);

      this.emit('run', {
        runIndex: i + 1,
        totalRuns: this.config.runs,
        latencyMs: result.latencyMs,
        ttftMs: result.ttftMs,
        tps: result.tps,
      });
    }

    // Calcul des statistiques
    const summary = this.calculateSummary(runs);

    const results: BenchmarkResults = {
      model: modelName,
      timestamp: new Date(),
      config: this.config,
      runs,
      summary,
    };

    this.emit('complete', results);
    return results;
  }

  /**
   * üìä Calculer les statistiques
   */
  private calculateSummary(runs: RunResult[]): BenchmarkSummary {
    const successful = runs.filter(r => !r.error);
    const latencies = successful.map(r => r.latencyMs);
    const ttfts = successful.map(r => r.ttftMs);
    const tpsValues = successful.map(r => r.tps);

    return {
      totalRuns: runs.length,
      successfulRuns: successful.length,
      failedRuns: runs.length - successful.length,

      latency: this.calculateStats(latencies),

      ttft: {
        ...this.calculateStats(ttfts),
        p50: this.percentile(ttfts, 0.50),
        p95: this.percentile(ttfts, 0.95),
        p99: this.percentile(ttfts, 0.99),
      },

      tps: this.calculateStats(tpsValues),

      throughput: successful.length / (latencies.reduce((a, b) => a + b, 0) / 1000),

      inputTokens: {
        total: successful.reduce((sum, r) => sum + r.inputTokens, 0),
        average: successful.reduce((sum, r) => sum + r.inputTokens, 0) / successful.length,
      },

      outputTokens: {
        total: successful.reduce((sum, r) => sum + r.outputTokens, 0),
        average: successful.reduce((sum, r) => sum + r.outputTokens, 0) / successful.length,
      },

      cost: this.calculateCost(successful),
    };
  }

  /**
   * üÜö Comparer deux benchmarks
   */
  compare(baseline: BenchmarkResults, current: BenchmarkResults): ComparisonResult {
    const ttftDiff = current.summary.ttft.avg - baseline.summary.ttft.avg;
    const tpsDiff = current.summary.tps.avg - baseline.summary.tps.avg;

    return {
      baseline: baseline.model,
      current: current.model,

      ttft: {
        baseline: baseline.summary.ttft.avg,
        current: current.summary.ttft.avg,
        diff: ttftDiff,
        percentChange: (ttftDiff / baseline.summary.ttft.avg) * 100,
        improved: ttftDiff < 0,
      },

      tps: {
        baseline: baseline.summary.tps.avg,
        current: current.summary.tps.avg,
        diff: tpsDiff,
        percentChange: (tpsDiff / baseline.summary.tps.avg) * 100,
        improved: tpsDiff > 0,
      },
    };
  }
}
```

**Karim** : "Et on peut automatiser ces tests ?"

**Lina** : "Oui, on peut les int√©grer dans la CI. √Ä chaque commit, on v√©rifie que les performances ne r√©gressent pas."

![Benchmark Results](../images/benchmark-results.svg)

---

## 19.9 üìù Schema Validator : Le Garde-fou du Structured Output

### 19.9.1 üò± Le Cauchemar du JSON Mal Form√©

*Marc d√©bugue un probl√®me en production.*

**Marc** *(frustr√©)* : "Le LLM a encore g√©n√©r√© du JSON invalide. Regarde √ßa."

```
I will use the read_file tool with the path "/tmp/test.txt" to read the contents.

{"tool": "read_file", "arguments": {"path": "/tmp/test.txt"},}
                                                            ^
                                                            trailing comma!
```

**Lina** : "C'est un probl√®me classique. Les LLM ne sont pas des g√©n√©rateurs JSON fiables."

**Marc** : "Comment on r√©sout √ßa ?"

### 19.9.2 üõ°Ô∏è Le SchemaValidator

![Schema Validator Flow](../images/schema-validator-flow.svg)

```typescript
// src/utils/schema-validator.ts

/**
 * üìù SchemaValidator - Validation JSON Schema avec tol√©rance aux erreurs
 *
 * Fonctionnalit√©s :
 * - Extraction de JSON depuis du texte
 * - Correction des erreurs communes (trailing commas)
 * - Coercion de types
 * - Valeurs par d√©faut
 */
export class SchemaValidator extends EventEmitter {
  private config: ValidatorConfig;

  constructor(config: Partial<ValidatorConfig> = {}) {
    super();
    this.config = {
      coerceTypes: config.coerceTypes ?? true,
      removeAdditional: config.removeAdditional ?? true,
      useDefaults: config.useDefaults ?? true,
      maxRetries: config.maxRetries ?? 3,
    };
  }

  /**
   * üîç Extraire du JSON depuis du texte
   */
  extractJSON(text: string): ExtractResult | null {
    // Essai 1 : JSON direct
    try {
      const json = JSON.parse(text);
      return { json, extracted: false };
    } catch {}

    // Essai 2 : Code block markdown
    const codeBlockMatch = text.match(/```(?:json)?\s*([\s\S]*?)```/);
    if (codeBlockMatch) {
      try {
        const json = JSON.parse(codeBlockMatch[1].trim());
        return { json, extracted: true };
      } catch {}
    }

    // Essai 3 : JSON imbriqu√© dans du texte
    const jsonMatch = text.match(/\{[\s\S]*\}|\[[\s\S]*\]/);
    if (jsonMatch) {
      // Correction des erreurs communes
      let candidate = jsonMatch[0];
      candidate = this.fixCommonErrors(candidate);

      try {
        const json = JSON.parse(candidate);
        return { json, extracted: true };
      } catch {}
    }

    return null;
  }

  /**
   * üîß Corriger les erreurs JSON communes
   */
  private fixCommonErrors(json: string): string {
    // Trailing commas
    json = json.replace(/,\s*([}\]])/g, '$1');

    // Single quotes ‚Üí double quotes
    json = json.replace(/'/g, '"');

    // Unquoted keys (simple cases)
    json = json.replace(/(\{|\,)\s*(\w+)\s*:/g, '$1"$2":');

    return json;
  }

  /**
   * ‚úÖ Valider et coercer les donn√©es
   */
  validate(data: unknown, schema: JSONSchema): ValidationResult {
    const errors: ValidationError[] = [];
    let coerced = false;

    const validated = this.validateNode(data, schema, '', errors, () => {
      coerced = true;
    });

    return {
      valid: errors.length === 0,
      data: validated,
      errors,
      coerced,
    };
  }

  /**
   * üéØ Valider une r√©ponse LLM compl√®te
   */
  validateResponse(response: string, schema: JSONSchema): ResponseValidation {
    // Extraire le JSON
    const extracted = this.extractJSON(response);

    if (!extracted) {
      return {
        valid: false,
        errors: [{ path: '', message: 'Could not extract JSON from response' }],
        raw: response,
      };
    }

    // Valider
    const validation = this.validate(extracted.json, schema);

    const result = {
      valid: validation.valid,
      data: validation.data,
      errors: validation.errors,
      raw: response,
      extracted: extracted.extracted,
      coerced: validation.coerced,
    };

    this.emit('validation', result);
    return result;
  }
}

/**
 * üìã Schemas pr√©d√©finis pour les cas courants
 */
export const TOOL_CALL_SCHEMA: JSONSchema = {
  type: 'object',
  properties: {
    tool: { type: 'string' },
    arguments: { type: 'object' },
    reasoning: { type: 'string' },
  },
  required: ['tool', 'arguments'],
};

export const ACTION_PLAN_SCHEMA: JSONSchema = {
  type: 'object',
  properties: {
    goal: { type: 'string' },
    steps: {
      type: 'array',
      items: {
        type: 'object',
        properties: {
          action: { type: 'string' },
          description: { type: 'string' },
        },
      },
    },
    estimatedSteps: { type: 'number' },
  },
  required: ['goal', 'steps'],
};
```

**Marc** : "Et la coercion de types, c'est quoi exactement ?"

**Lina** : "Si le LLM g√©n√®re `"123"` alors qu'on attend un nombre, on convertit automatiquement en `123`. Pareil pour les bool√©ens ‚Äî `"true"` devient `true`."

**Marc** : "√áa √©vite combien d'erreurs ?"

**Lina** : "En production ? Environ 15% des r√©ponses sont coerc√©es. Sans √ßa, ce serait des erreurs."

---

## üåÖ √âpilogue

*Deux mois plus tard. R√©union trimestrielle.*

**Karim** *(pr√©sentant les r√©sultats)* : "Les chiffres sont impressionnants. Notre facture API est pass√©e de 2,500 √† 600 dollars par mois."

**Lina** : "Et les performances locales sont stables. TTFT moyen de 180ms, 45 tokens par seconde."

**Marc** : "Le speculative decoding a √©t√© un game changer. On g√©n√®re du code 2.5x plus vite qu'avant."

*Sophie, la d√©veloppeuse junior, l√®ve la main.*

**Sophie** : "J'ai une question. Hier, j'ai test√© Code Buddy sur mon laptop personnel. Un MacBook Air M2. Et √ßa marchait. Comment c'est possible sans GPU NVIDIA ?"

**Lina** *(souriant)* : "Le GPUMonitor d√©tecte Apple Silicon et utilise Metal pour l'acc√©l√©ration. C'est transparent."

**Sophie** : "Et la qualit√© ?"

**Lina** : "Identique. Les mod√®les Qwen et Llama sont optimis√©s pour ARM. Tu peux faire tourner un 7B quantifi√© sur 8 Go de RAM unifi√©e."

**Karim** : "C'est exactement ce qu'on visait. Une infrastructure locale qui fonctionne pour tout le monde."

*Marc regarde son √©cran, pensif.*

**Marc** : "Il y a un truc qui me tracasse. Tout √ßa fonctionne super bien pour les t√¢ches simples. Mais pour les t√¢ches vraiment complexes..."

**Lina** : "On envoie toujours au cloud. Claude pour l'architecture, GPT-4 pour la s√©curit√©."

**Marc** : "Mais si on pouvait faire √ßa aussi en local ?"

*Un silence s'installe.*

**Karim** : "Tu veux dire... des agents complexes, enti√®rement locaux ?"

**Marc** : "Des mod√®les 70B. Du MoE. De la sp√©cialisation."

**Lina** *(les yeux brillants)* : "J'ai lu un papier r√©cemment. Sur le fine-tuning efficace avec LoRA. Et les techniques de serveur de mod√®les comme vLLM..."

*Elle ouvre son laptop.*

**Lina** : "On pourrait construire quelque chose de vraiment int√©ressant."

---

## üìä Tableau Synth√©tique ‚Äî Chapitre 19

| Aspect | D√©tails |
|--------|---------|
| **Titre** | Infrastructure LLM Local |
| **GPU Monitor** | Surveillance VRAM multi-vendor, recommandations de layers |
| **Ollama Embeddings** | Embeddings gratuits, `nomic-embed-text` 768 dims |
| **HNSW Store** | Recherche vectorielle O(log N), TypeScript pur |
| **Model Hub** | T√©l√©chargement HuggingFace, gestion quantizations |
| **KV-Cache** | Quantification q8_0 = -50% m√©moire |
| **Speculative** | Draft + Target = 2-3x speedup |
| **Benchmark** | TTFT, TPS, p95, comparaison de mod√®les |
| **Schema Validator** | Extraction JSON, coercion, structured output fiable |

---

## üìù Points Cl√©s

| Concept | Ic√¥ne | Description | Impact |
|---------|:-----:|-------------|--------|
| **GPU Monitor** | üñ•Ô∏è | D√©tection VRAM, recommandations | √âvite crashs OOM |
| **Ollama** | üßÆ | Embeddings locaux | -100% co√ªt embeddings |
| **HNSW** | üîç | Recherche vectorielle pure TS | Z√©ro d√©pendances |
| **Model Hub** | üì¶ | Gestion mod√®les HuggingFace | T√©l√©chargement simplifi√© |
| **KV-Cache** | üß† | Quantification du cache | +50% contexte |
| **Speculative** | ‚ö° | Draft + Verify | 2-3x speedup |
| **Benchmark** | üìä | TTFT, TPS, p95 | Optimisation guid√©e |
| **Schema** | üìù | Validation structured output | Tool calling fiable |

---

## üèãÔ∏è Exercices

### Exercice 1 : üñ•Ô∏è Multi-GPU
Impl√©mentez la d√©tection et l'utilisation de plusieurs GPUs :
- R√©partition des layers entre GPUs
- Load balancing intelligent
- Failover automatique

### Exercice 2 : üîç Index Hybrid
Cr√©ez un index hybride HNSW + BM25 :
- Recherche s√©mantique (HNSW)
- Recherche lexicale (BM25)
- Fusion des scores avec RRF

### Exercice 3 : ‚ö° Batched Speculative
Impl√©mentez le speculative decoding en batch :
- Plusieurs requ√™tes simultan√©es
- Partage du mod√®le target
- Maximisation du throughput

### Exercice 4 : üìä Dashboard Temps R√©el
Construisez un dashboard de monitoring :
- VRAM en temps r√©el
- M√©triques de benchmark live
- Alertes sur d√©gradation

---

## üß≠ Navigation

| Pr√©c√©dent | Suivant |
|:---------:|:-------:|
| [‚Üê Chapitre 18 : Productivit√© CLI](../18-productivite-cli.md) | [Annexe A : Transformers ‚Üí](../annexe-a-transformers.md) |

---

**√Ä suivre** : *Annexe A ‚Äî Architecture Transformers*

*Comment fonctionne r√©ellement un LLM ? Qu'est-ce que l'attention ? Pourquoi le KV-cache existe-t-il ? Plongeons dans les fondamentaux de l'architecture Transformer, des embeddings aux couches de sortie.*
