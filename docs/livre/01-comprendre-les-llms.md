# ğŸ§  Chapitre 1 : Comprendre les Large Language Models

---

## ğŸ¬ ScÃ¨ne d'ouverture : La Question Fondamentale

*Un mardi soir, dans un cafÃ© prÃ¨s du campus universitaire...*

Lina fixait son Ã©cran, perplexe. Elle venait de passer trois heures Ã  interagir avec ChatGPT, lui demandant d'expliquer du code, de gÃ©nÃ©rer des tests, de suggÃ©rer des refactorisations. Les rÃ©sultats Ã©taient tantÃ´t brillants, tantÃ´t absurdes.

â€” "Comment Ã§a peut Ãªtre si intelligent et si stupide Ã  la fois ?" murmura-t-elle.

Son ami Marcus, doctorant en machine learning, s'assit Ã  cÃ´tÃ© d'elle avec son cafÃ©.

â€” "Tu sais comment Ã§a fonctionne, un LLM ?"

Lina haussa les Ã©paules.

â€” "Vaguement. Des rÃ©seaux de neurones, beaucoup de donnÃ©es, quelque chose avec l'attention..."

Marcus sourit.

â€” "C'est un bon dÃ©but. Mais si tu veux vraiment construire des outils qui utilisent ces modÃ¨les, tu dois comprendre ce qu'ils sont *vraiment*. Pas la version marketing. La vraie mÃ©canique."

Il sortit un carnet et commenÃ§a Ã  dessiner.

â€” "Laisse-moi te raconter une histoire. Elle commence en 2017, dans les bureaux de Google..."

---

## ğŸ“œ 1.1 Une BrÃ¨ve Histoire des ModÃ¨les de Langage

### 1.1.1 Avant les Transformers : L'Ãˆre des Approches SÃ©quentielles

Pour comprendre pourquoi les LLMs actuels sont si puissants, il faut d'abord comprendre ce qui existait avant â€” et pourquoi c'Ã©tait insuffisant.

Pendant des dÃ©cennies, le traitement automatique du langage naturel (NLP) reposait sur des approches statistiques. Les modÃ¨les n-grammes, par exemple, prÃ©disaient le mot suivant en comptant les frÃ©quences d'apparition dans un corpus.

> ğŸ’¡ **Exemple** : Si le modÃ¨le avait vu "le chat dort sur le" mille fois suivi de "canapÃ©" et seulement dix fois suivi de "toit", il prÃ©dirait "canapÃ©".

Cette approche avait un dÃ©faut fondamental : elle ne capturait que des **dÃ©pendances locales**. Un modÃ¨le 5-gramme ne pouvait "voir" que les quatre mots prÃ©cÃ©dents. Or, le langage humain est plein de dÃ©pendances Ã  longue distance :

> "Le dÃ©veloppeur qui avait passÃ© trois ans Ã  travailler sur ce projet, malgrÃ© les difficultÃ©s rencontrÃ©es avec l'Ã©quipe de management et les contraintes budgÃ©taires imposÃ©es par la direction, **Ã©tait** finalement satisfait du rÃ©sultat."

Le verbe "Ã©tait" doit s'accorder avec "Le dÃ©veloppeur" â€” un mot situÃ© Ã  plus de trente tokens de distance ! Aucun modÃ¨le n-gramme ne pouvait capturer cette relation.

### 1.1.2 Les RÃ©seaux RÃ©currents : Une Fausse Bonne IdÃ©e

Dans les annÃ©es 2010, les rÃ©seaux de neurones rÃ©currents (RNN) et leurs variantes (LSTM, GRU) ont apportÃ© une amÃ©lioration significative.

| ğŸ“Š Comparaison | N-grammes | RNN/LSTM |
|:--------------|:----------|:---------|
| **MÃ©moire** | FenÃªtre fixe (3-5 mots) | ThÃ©oriquement illimitÃ©e |
| **Contexte** | Local uniquement | Peut propager l'information |
| **ParallÃ©lisation** | âœ… Facile | âŒ SÃ©quentiel obligatoire |
| **EntraÃ®nement** | Rapide | Lent |

L'idÃ©e des RNN Ã©tait Ã©lÃ©gante : au lieu de regarder une fenÃªtre fixe de mots, le rÃ©seau maintenait un "Ã©tat cachÃ©" qui se propageait d'un mot au suivant, thÃ©oriquement capable de "se souvenir" d'informations arbitrairement lointaines.

En pratique, cette promesse n'Ã©tait que partiellement tenue. Les RNN souffraient de deux problÃ¨mes majeurs :

| âš ï¸ ProblÃ¨me | Description | ConsÃ©quence |
|:-----------|:------------|:------------|
| **Gradient Ã©vanescent** | Le signal d'erreur diminue exponentiellement Ã  travers la chaÃ®ne | Le rÃ©seau "oublie" les dÃ©pendances lointaines |
| **SÃ©quentialitÃ©** | Traitement mot par mot, dans l'ordre | Impossible de parallÃ©liser sur GPU |

### 1.1.3 âš¡ 2017 : "Attention Is All You Need"

En juin 2017, une Ã©quipe de Google publia un article au titre provocateur : **"Attention Is All You Need"**. Les auteurs proposaient une architecture radicalement diffÃ©rente : le **Transformer**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸ¯ L'IDÃ‰E RÃ‰VOLUTIONNAIRE                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚   AVANT (RNN) :    motâ‚ â†’ motâ‚‚ â†’ motâ‚ƒ â†’ motâ‚„ â†’ motâ‚…        â”‚
â”‚                    (sÃ©quentiel, lent)                       â”‚
â”‚                                                             â”‚
â”‚   APRÃˆS (Transformer) :                                     â”‚
â”‚                                                             â”‚
â”‚                    motâ‚ â†â”€â”€â”€â”€â”€â”€â†’ motâ‚‚                       â”‚
â”‚                      â†•    â•²  â•±     â†•                        â”‚
â”‚                    motâ‚ƒ â†â”€â”€â•³â”€â”€â”€â†’ motâ‚„                       â”‚
â”‚                      â†•    â•±  â•²     â†•                        â”‚
â”‚                    motâ‚… â†â”€â”€â”€â”€â”€â”€â†’ motâ‚†                       â”‚
â”‚                                                             â”‚
â”‚                    (parallÃ¨le, tous connectÃ©s)              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

L'idÃ©e centrale Ã©tait audacieuse : et si on abandonnait complÃ¨tement la rÃ©currence ? Au lieu de traiter les mots sÃ©quentiellement, pourquoi ne pas les traiter **tous en parallÃ¨le**, en utilisant uniquement des mÃ©canismes d'attention pour capturer les relations entre eux ?

| ğŸ“ˆ RÃ©sultats | RNN (LSTM) | Transformer |
|:------------|:-----------|:------------|
| **Vitesse d'entraÃ®nement** | Baseline | **3-10x plus rapide** |
| **QualitÃ© (BLEU)** | 25.8 | **28.4** |
| **DÃ©pendances longues** | âš ï¸ Difficile | âœ… Native |
| **ParallÃ©lisation GPU** | âŒ LimitÃ©e | âœ… Massive |

Un an plus tard, Google dÃ©voilait **BERT**, et OpenAI prÃ©sentait **GPT**. L'Ã¨re des LLMs venait de commencer.

---

## ğŸ”¬ 1.2 L'Anatomie d'un Transformer

Pour construire des agents efficaces, il ne suffit pas de savoir que les Transformers "fonctionnent bien". Il faut comprendre *comment* ils fonctionnent.

### 1.2.1 âœ‚ï¸ La Tokenisation : DÃ©couper le Langage

Avant mÃªme d'entrer dans le rÃ©seau de neurones, le texte doit Ãªtre converti en nombres. Cette Ã©tape, appelÃ©e **tokenisation**, est plus subtile qu'il n'y paraÃ®t.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ğŸ”¤ PROCESSUS DE TOKENISATION                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Texte : "Le dÃ©veloppeur mass bien"                         â”‚
â”‚                    â”‚                                        â”‚
â”‚                    â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚       Tokenizer (BPE/WordPiece)     â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                    â”‚                                        â”‚
â”‚                    â–¼                                        â”‚
â”‚  Tokens : ["Le", "dÃ©", "velopp", "eur", "code", "bien"]    â”‚
â”‚                    â”‚                                        â”‚
â”‚                    â–¼                                        â”‚
â”‚  IDs : [453, 8721, 34502, 2174, 1825, 3901]                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

La solution moderne est le **Byte-Pair Encoding (BPE)**. L'idÃ©e est de dÃ©couper le texte en "sous-mots" â€” des fragments qui peuvent Ãªtre combinÃ©s pour former n'importe quel mot.

| ğŸŒ Langue | Texte | Tokens | Ratio |
|:---------|:------|:------:|:-----:|
| ğŸ‡¬ğŸ‡§ Anglais | "The developer writes code" | 4 | 1.0x |
| ğŸ‡«ğŸ‡· FranÃ§ais | "Le dÃ©veloppeur Ã©crit du code" | 7 | 1.4x |
| ğŸ‡¯ğŸ‡µ Japonais | "é–‹ç™ºè€…ã¯ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã" | 9 | 2.25x |
| ğŸ‡¨ğŸ‡³ Chinois | "å¼€å‘äººå‘˜ç¼–å†™ä»£ç " | 8 | 2.0x |

> âš ï¸ **ConsÃ©quence importante** : Les langues non-anglaises consomment plus de tokens, donc coÃ»tent plus cher !

**Implications pour les agents de dÃ©veloppement :**

| Impact | Description |
|:-------|:------------|
| ğŸ’° **CoÃ»t** | Le code avec des noms longs (ex: `calculateTotalAmountWithTax`) coÃ»te plus cher |
| ğŸ”¢ **Comptage** | Les LLMs ne "voient" pas les caractÃ¨res individuels â€” mauvais pour compter les lettres |
| ğŸ“ **Contexte** | Un fichier de 1000 lignes peut consommer 10K+ tokens |

### 1.2.2 ğŸ¯ Les Embeddings : Donner du Sens aux Nombres

Une fois tokenisÃ©, chaque identifiant est converti en un **vecteur de nombres rÃ©els** â€” son embedding. Dans GPT-4, ces vecteurs ont plusieurs milliers de dimensions.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ§­ ESPACE DES EMBEDDINGS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚                        ğŸ‘‘ roi                               â”‚
â”‚                       â•±                                     â”‚
â”‚                      â•±  (direction "royautÃ©")               â”‚
â”‚                     â•±                                       â”‚
â”‚        ğŸ‘¨ homme â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ‘© femme               â”‚
â”‚                     â•²                                       â”‚
â”‚                      â•²  (direction "royautÃ©")               â”‚
â”‚                       â•²                                     â”‚
â”‚                        ğŸ‘¸ reine                             â”‚
â”‚                                                             â”‚
â”‚   Formule magique :                                         â”‚
â”‚   embedding(roi) - embedding(homme) + embedding(femme)      â”‚
â”‚                    â‰ˆ embedding(reine)                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Cette propriÃ©tÃ© n'est pas programmÃ©e explicitement â€” elle **Ã©merge** de l'entraÃ®nement. Le modÃ¨le "dÃ©couvre" que les mots apparaissant dans des contextes similaires devraient avoir des embeddings proches.

Pour le code, c'est prÃ©cieux :

```
embedding("array.push")    â‰ˆ  embedding("list.append")
embedding("console.log")   â‰ˆ  embedding("print")
embedding("async/await")   â‰ˆ  embedding("Promise")
```

C'est ce qui permet aux systÃ¨mes de **RAG** de trouver du code pertinent mÃªme quand les mots exacts diffÃ¨rent !

### 1.2.3 ğŸ‘ï¸ L'Attention : Le CÅ“ur du Transformer

Le mÃ©canisme d'attention est ce qui distingue fondamentalement les Transformers. Pour le comprendre, une analogie :

> ğŸ“– **Analogie du roman policier**
>
> Imaginez que vous lisez un polar. Ã€ la page 200, le dÃ©tective rÃ©vÃ¨le : "le majordome Ã©tait le coupable". Pour comprendre, votre cerveau rappelle instantanÃ©ment :
> - Qui est le majordome (page 15)
> - Les indices subtils (pages 45, 78, 123)
> - Le contexte de l'enquÃªte
>
> Vous ne relisez pas tout â€” votre cerveau *sait* quelles informations sont pertinentes.

L'attention fonctionne de maniÃ¨re similaire. Pour chaque token, le modÃ¨le calcule un score de "pertinence" avec tous les autres tokens :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ğŸ¯ MÃ‰CANISME D'ATTENTION                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Pour chaque token, on calcule 3 vecteurs :                 â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ Query Q â”‚  "Quelle information je cherche ?"             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚  Key K  â”‚  "Quelle information je peux fournir ?"        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                â”‚
â”‚  â”‚ Value V â”‚  "Voici l'information proprement dite"         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                â”‚
â”‚                                                             â”‚
â”‚  Score d'attention = softmax(Q Â· K^T / âˆšd) Ã— V              â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Exemple concret avec du code :**

```python
def calculate_total(items):
    total = 0
    for item in items:
        total += item.price  # â† Quand on traite ce token...
    return total
```

| Token traitÃ© | Regarde vers... | Pourquoi ? |
|:-------------|:----------------|:-----------|
| `item.price` | `item` (ligne 3) | Comprendre le type |
| `item.price` | `items` (signature) | Structure de donnÃ©es |
| `item.price` | `total +=` | Contexte d'utilisation |
| `item.price` | `calculate_total` | Intention de la fonction |

Sans attention, le modÃ¨le ne verrait que `item.price` isolÃ©ment, sans contexte !

### 1.2.4 ğŸ­ L'Attention Multi-TÃªtes : Plusieurs Perspectives

Un raffinement crucial : au lieu d'un seul mÃ©canisme d'attention, le modÃ¨le en a **plusieurs** (32 Ã  128) fonctionnant en parallÃ¨le.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ­ ATTENTION MULTI-TÃŠTES                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Phrase : "Le programme Python que Marie a Ã©crit hier       â”‚
â”‚            fonctionne parfaitement."                        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ ğŸ”· TÃªte 1    â”‚  â”‚ ğŸ”¶ TÃªte 2    â”‚  â”‚ ğŸ”· TÃªte 3    â”‚      â”‚
â”‚  â”‚  Syntaxique  â”‚  â”‚  SÃ©mantique  â”‚  â”‚  Temporelle  â”‚      â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤      â”‚
â”‚  â”‚ programme â†”  â”‚  â”‚ Python â†”     â”‚  â”‚ hier â†”       â”‚      â”‚
â”‚  â”‚ fonctionne   â”‚  â”‚ programme    â”‚  â”‚ a Ã©crit      â”‚      â”‚
â”‚  â”‚ (sujet-verbe)â”‚  â”‚ (langage)    â”‚  â”‚ (temps)      â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ ğŸ”¶ TÃªte 4    â”‚  â”‚ ğŸ”· TÃªte 5    â”‚  ...jusqu'Ã  128        â”‚
â”‚  â”‚ Attribution  â”‚  â”‚ CorÃ©fÃ©rence  â”‚                        â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                        â”‚
â”‚  â”‚ Marie â†”      â”‚  â”‚ que â†”        â”‚                        â”‚
â”‚  â”‚ a Ã©crit      â”‚  â”‚ programme    â”‚                        â”‚
â”‚  â”‚ (qui fait)   â”‚  â”‚ (rÃ©fÃ¨re Ã )   â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Chaque tÃªte se spÃ©cialise dans un type de relation, et leurs sorties sont combinÃ©es pour former une reprÃ©sentation riche.

### 1.2.5 ğŸ“š Les Couches : Profondeur et Abstraction

Un Transformer n'a pas qu'une seule couche d'attention â€” il en a des dizaines :

| ğŸ¤– ModÃ¨le | Couches | ParamÃ¨tres |
|:---------|:-------:|:----------:|
| GPT-2 | 12-48 | 117M - 1.5B |
| GPT-3 | 96 | 175B |
| GPT-4 | ~120 (estimÃ©) | ~1.7T (estimÃ©) |
| Claude 3 | ? | ? |
| Grok-2 | ? | ? |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ“Š HIÃ‰RARCHIE DES COUCHES                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Couches hautes (80-120)                                    â”‚
â”‚  â””â”€ ğŸ¯ Concepts abstraits, intentions, raisonnement         â”‚
â”‚                                                             â”‚
â”‚  Couches moyennes (30-80)                                   â”‚
â”‚  â””â”€ ğŸ”— Relations sÃ©mantiques, corÃ©fÃ©rences                  â”‚
â”‚                                                             â”‚
â”‚  Couches basses (1-30)                                      â”‚
â”‚  â””â”€ ğŸ“ Syntaxe, grammaire, patterns locaux                  â”‚
â”‚                                                             â”‚
â”‚  EntrÃ©e                                                     â”‚
â”‚  â””â”€ ğŸ”¤ Tokens bruts                                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implication pratique** : Quand un LLM "ne comprend pas", le problÃ¨me peut Ãªtre Ã  diffÃ©rents niveaux :

| Niveau | SymptÃ´me | Solution |
|:-------|:---------|:---------|
| ğŸ”¤ Bas | Ne reconnaÃ®t pas la syntaxe | Reformuler, utiliser un format standard |
| ğŸ”— Moyen | Perd les rÃ©fÃ©rences | Ajouter du contexte, rÃ©pÃ©ter les Ã©lÃ©ments clÃ©s |
| ğŸ¯ Haut | Ne saisit pas l'intention | DÃ©composer la tÃ¢che, Chain-of-Thought |

---

## âš™ï¸ 1.3 Comment un LLM GÃ©nÃ¨re du Texte

### 1.3.1 ğŸ² La PrÃ©diction du Token Suivant

Au cÅ“ur de tout LLM gÃ©nÃ©ratif se trouve une tÃ¢che d'une simplicitÃ© trompeuse : **prÃ©dire le token suivant**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ¯ PRÃ‰DICTION NEXT-TOKEN                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  EntrÃ©e : "Le dÃ©veloppeur a corrigÃ© le"                     â”‚
â”‚                                                             â”‚
â”‚  Sortie (distribution de probabilitÃ©) :                     â”‚
â”‚                                                             â”‚
â”‚  bug â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  23%               â”‚
â”‚  problÃ¨me â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  18%               â”‚
â”‚  code â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  12%               â”‚
â”‚  fichier â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   8%               â”‚
â”‚  test â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   5%               â”‚
â”‚  ...                                                        â”‚
â”‚  Ã©lÃ©phant â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  0.001%            â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> âš ï¸ **Distinction cruciale** : Le modÃ¨le ne "sait" pas ce qui vient ensuite â€” il calcule ce qui serait *probable* Ã©tant donnÃ© son entraÃ®nement. Il reproduit ce qui est **probable**, pas nÃ©cessairement ce qui est **correct**.

### 1.3.2 ğŸŒ¡ï¸ L'Ã‰chantillonnage : Choisir Parmi les Possibles

Une fois la distribution calculÃ©e, il faut choisir un token. Plusieurs stratÃ©gies :

| ğŸ›ï¸ StratÃ©gie | Description | Usage |
|:-------------|:------------|:------|
| **Greedy** | Toujours le plus probable | âš ï¸ RÃ©pÃ©titif, ennuyeux |
| **Random** | Tirage selon probabilitÃ©s | âš ï¸ Parfois incohÃ©rent |
| **Temperature** | Aplatit/accentue la distribution | âœ… ContrÃ´le crÃ©ativitÃ© |
| **Top-p** | Garde les tokens jusqu'Ã  p% cumulÃ© | âœ… Ã‰quilibre variÃ©tÃ©/cohÃ©rence |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ğŸŒ¡ï¸ EFFET DE LA TEMPÃ‰RATURE                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Temperature = 0.1 (conservateur)                           â”‚
â”‚  bug â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  95%               â”‚
â”‚  problÃ¨me â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   4%               â”‚
â”‚  autres â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   1%               â”‚
â”‚                                                             â”‚
â”‚  Temperature = 0.7 (Ã©quilibrÃ©)                              â”‚
â”‚  bug â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  45%                â”‚
â”‚  problÃ¨me â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  30%                â”‚
â”‚  code â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  15%                â”‚
â”‚  autres â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  10%                â”‚
â”‚                                                             â”‚
â”‚  Temperature = 1.2 (crÃ©atif)                                â”‚
â”‚  bug â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  25%                â”‚
â”‚  problÃ¨me â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  20%                â”‚
â”‚  code â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  15%                â”‚
â”‚  fichier â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  10%                â”‚
â”‚  autres â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  30%                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Recommandations pour les agents :**

| ğŸ¯ TÃ¢che | TempÃ©rature | Pourquoi |
|:---------|:-----------:|:---------|
| GÃ©nÃ©ration de code | 0.2 - 0.4 | PrÃ©cision syntaxique |
| Refactoring | 0.3 - 0.5 | CohÃ©rence avec l'existant |
| Documentation | 0.5 - 0.7 | Style naturel |
| Brainstorming | 0.7 - 0.9 | CrÃ©ativitÃ© |
| Noms de variables | 0.6 - 0.8 | VariÃ©tÃ© mais pertinence |

### 1.3.3 ğŸ”„ L'Autoregression : Un Token Ã  la Fois

Les LLMs gÃ©nÃ©ratifs sont **autorÃ©gressifs** : ils gÃ©nÃ¨rent un token, l'ajoutent au contexte, puis gÃ©nÃ¨rent le suivant.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ”„ GÃ‰NÃ‰RATION AUTORÃ‰GRESSIVE                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Ã‰tape 1: "function add(a, b) {"                            â”‚
â”‚           â†’ prÃ©dit: "return"                                â”‚
â”‚                                                             â”‚
â”‚  Ã‰tape 2: "function add(a, b) { return"                     â”‚
â”‚           â†’ prÃ©dit: "a"                                     â”‚
â”‚                                                             â”‚
â”‚  Ã‰tape 3: "function add(a, b) { return a"                   â”‚
â”‚           â†’ prÃ©dit: "+"                                     â”‚
â”‚                                                             â”‚
â”‚  Ã‰tape 4: "function add(a, b) { return a +"                 â”‚
â”‚           â†’ prÃ©dit: "b"                                     â”‚
â”‚                                                             â”‚
â”‚  Ã‰tape 5: "function add(a, b) { return a + b"               â”‚
â”‚           â†’ prÃ©dit: ";"                                     â”‚
â”‚                                                             â”‚
â”‚  Ã‰tape 6: "function add(a, b) { return a + b;"              â”‚
â”‚           â†’ prÃ©dit: "}"                                     â”‚
â”‚                                                             â”‚
â”‚  âœ… TerminÃ© !                                                â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> âš ï¸ **ConsÃ©quence importante** : Le modÃ¨le ne peut pas "revenir en arriÃ¨re". Une erreur au dÃ©but influence tout le reste. C'est pourquoi les techniques comme **Chain-of-Thought** sont si efficaces â€” elles permettent de poser les bases d'un raisonnement avant la rÃ©ponse finale.

---

## âš ï¸ 1.4 Les Limites Fondamentales des LLMs

Comprendre les limites n'est pas du pessimisme â€” c'est une **nÃ©cessitÃ©** pour construire des agents robustes.

### 1.4.1 ğŸ‘» Les Hallucinations : Quand le Probable N'est Pas le Vrai

Le terme "hallucination" dÃ©signe les cas oÃ¹ un LLM gÃ©nÃ¨re des informations **fausses avec confiance**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ‘» ANATOMIE D'UNE HALLUCINATION                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Vous : "Quelle est la fonction lodash.deepMergeRecursive?"â”‚
â”‚                                                             â”‚
â”‚  LLM : "La fonction lodash.deepMergeRecursive permet de     â”‚
â”‚         fusionner rÃ©cursivement des objets imbriquÃ©s.       â”‚
â”‚         Elle prend deux arguments : l'objet cible et        â”‚
â”‚         l'objet source..."                                  â”‚
â”‚                                                             â”‚
â”‚  âŒ CETTE FONCTION N'EXISTE PAS !                           â”‚
â”‚                                                             â”‚
â”‚  Mais la rÃ©ponse est plausible car :                        â”‚
â”‚  âœ“ lodash existe                                            â”‚
â”‚  âœ“ Elle a des fonctions de merge                            â”‚
â”‚  âœ“ "deepMergeRecursive" semble logique                      â”‚
â”‚  âœ“ L'explication suit le pattern de vraie documentation    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Types d'hallucinations dans le code :**

| ğŸ‘» Type | Exemple | Danger |
|:--------|:--------|:------:|
| **APIs inventÃ©es** | `array.deepClone()` au lieu de `structuredClone()` | ğŸ”´ Ã‰levÃ© |
| **Imports fantÃ´mes** | `import { useQuery } from 'react-query'` (ancien nom) | ğŸŸ¡ Moyen |
| **ParamÃ¨tres faux** | `fs.readFile(path, 'utf-8', { recursive: true })` | ğŸ”´ Ã‰levÃ© |
| **Comportements inventÃ©s** | "Cette fonction retourne null si..." (faux) | ğŸ”´ Ã‰levÃ© |

> ğŸ’¡ **Solution** : Ne pas espÃ©rer Ã©liminer les hallucinations â€” concevoir des systÃ¨mes qui les **dÃ©tectent et corrigent**. C'est l'une des raisons d'Ãªtre des agents !

### 1.4.2 ğŸ“ La FenÃªtre de Contexte : Une MÃ©moire LimitÃ©e

Un LLM ne voit que sa **fenÃªtre de contexte** :

| ğŸ¤– ModÃ¨le | FenÃªtre | â‰ˆ Pages de texte |
|:---------|:-------:|:----------------:|
| GPT-3.5 | 4K - 16K | 3 - 12 pages |
| GPT-4 | 8K - 128K | 6 - 100 pages |
| Claude 3 | 200K | ~150 pages |
| Grok-2 | 128K | ~100 pages |

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ“ LA FENÃŠTRE DE CONTEXTE                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ â”‚  â”‚
â”‚  â”‚      INVISIBLE (au-delÃ  de la fenÃªtre)              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                   â”‚
â”‚                         â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚  â”‚
â”‚  â”‚      VISIBLE (dans la fenÃªtre de contexte)          â”‚  â”‚
â”‚  â”‚                                                      â”‚  â”‚
â”‚  â”‚  - System prompt                                     â”‚  â”‚
â”‚  â”‚  - Historique rÃ©cent                                 â”‚  â”‚
â”‚  â”‚  - Fichiers injectÃ©s                                 â”‚  â”‚
â”‚  â”‚  - Message actuel                                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                             â”‚
â”‚  âš ï¸ Plus le contexte est long, plus l'infÃ©rence coÃ»te !    â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Solutions des agents modernes :**

| ğŸ› ï¸ Technique | Description | Chapitre |
|:-------------|:------------|:--------:|
| **RAG** | RÃ©cupÃ©rer dynamiquement l'info pertinente | Ch. 7-8 |
| **Compression** | RÃ©sumer les informations moins importantes | Ch. 9 |
| **MÃ©moire externe** | Stocker dans une DB consultable | Ch. 14 |

### 1.4.3 ğŸ§  Le Raisonnement : Apparence vs. RÃ©alitÃ©

Les LLMs *semblent* raisonner. Mais ce "raisonnement" est-il comparable au raisonnement humain ?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ§  RAISONNEMENT : MYTHE VS RÃ‰ALITÃ‰             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  CE QU'ON CROIT :                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ProblÃ¨me â†’ Analyse â†’ Logique â†’ DÃ©duction â†’ RÃ©ponse  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  CE QUI SE PASSE VRAIMENT :                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ ProblÃ¨me â†’ Pattern matching â†’ GÃ©nÃ©ration plausible  â”‚   â”‚
â”‚  â”‚            (ressemble Ã  X     (vu pendant           â”‚   â”‚
â”‚  â”‚             dans training)     l'entraÃ®nement)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Preuves de cette distinction :**

| Test | RÃ©sultat | Implication |
|:-----|:---------|:------------|
| ProblÃ¨me classique, formulation standard | âœ… RÃ©ussit | Pattern reconnu |
| MÃªme problÃ¨me, formulation inhabituelle | âŒ Ã‰choue souvent | Pattern non reconnu |
| ProblÃ¨me simple mais inÃ©dit | âŒ Peut Ã©chouer | Pas de pattern |
| ProblÃ¨me complexe mais "classique" | âœ… Peut rÃ©ussir | Pattern mÃ©morisÃ© |

> ğŸ’¡ C'est pourquoi le **prompt engineering** fonctionne : il reformule le problÃ¨me sous une forme que le modÃ¨le reconnaÃ®t !

### 1.4.4 â° La Connaissance : FigÃ©e dans le Temps

Un LLM est entraÃ®nÃ© sur un corpus avec une **date de coupure**. Tout ce qui vient aprÃ¨s lui est inconnu.

| ğŸ¤– ModÃ¨le | Date de coupure | Ne connaÃ®t pas... |
|:---------|:----------------|:------------------|
| GPT-4 (original) | Sept 2021 | GPT-4 lui-mÃªme ! |
| GPT-4 Turbo | Avril 2023 | Claude 3, Grok-2 |
| Claude 3 | DÃ©but 2024 | ActualitÃ©s rÃ©centes |
| Grok-2 | ? | ? |

**ProblÃ¨mes pour le dÃ©veloppement :**

| âš ï¸ Risque | Exemple |
|:----------|:--------|
| APIs obsolÃ¨tes | SuggÃ¨re `componentWillMount` (dÃ©prÃ©ciÃ© React 16.3) |
| Packages renommÃ©s | `react-query` â†’ `@tanstack/react-query` |
| Failles non connues | SuggÃ¨re une version vulnÃ©rable |
| Nouvelles features | Ignore les derniÃ¨res additions au langage |

> ğŸ’¡ **Solution** : Augmenter le LLM avec des sources actuelles â€” documentation rÃ©cente, recherche web, exemples Ã  jour. C'est le rÃ´le de l'agent !

---

## ğŸ¤– 1.5 Du LLM Ã  l'Agent : Pourquoi l'Enrobage Compte

### 1.5.1 Le LLM Nu : Puissant mais Incomplet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ”’ LIMITATIONS DU LLM SEUL                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  âŒ Ne peut pas exÃ©cuter de code                            â”‚
â”‚  âŒ Ne peut pas accÃ©der Ã  Internet                          â”‚
â”‚  âŒ Ne peut pas lire/Ã©crire des fichiers                    â”‚
â”‚  âŒ Ne peut pas vÃ©rifier ses affirmations                   â”‚
â”‚  âŒ Ne peut pas apprendre de ses erreurs                    â”‚
â”‚  âŒ Ne peut pas interagir avec des APIs                     â”‚
â”‚                                                             â”‚
â”‚  ğŸ­ Analogie : Un expert brillant enfermÃ© dans une piÃ¨ce   â”‚
â”‚     sans fenÃªtre, sans tÃ©lÃ©phone, sans ordinateur.          â”‚
â”‚     Il peut rÃ©pondre... mais pas AGIR.                      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.5.2 âš¡ L'Agent : Le LLM AugmentÃ©

Un agent transforme le LLM en acteur capable d'agir :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ”„ LA BOUCLE REACT                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  LLM NU :                                                   â”‚
â”‚  Question â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º RÃ©ponse       â”‚
â”‚                                                (peut-Ãªtre    â”‚
â”‚                                                 fausse)      â”‚
â”‚                                                             â”‚
â”‚  AGENT :                                                    â”‚
â”‚                                                             â”‚
â”‚  Question                                                   â”‚
â”‚     â”‚                                                       â”‚
â”‚     â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ ğŸ§  Think â”‚â”€â”€â”€â–ºâ”‚ ğŸ”§ Act   â”‚â”€â”€â”€â–ºâ”‚ ğŸ‘ï¸ Observeâ”‚             â”‚
â”‚  â”‚ RÃ©flÃ©chirâ”‚    â”‚ Utiliser â”‚    â”‚ Voir le  â”‚              â”‚
â”‚  â”‚          â”‚â—„â”€â”€â”€â”‚ un outil â”‚â—„â”€â”€â”€â”‚ rÃ©sultat â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚       â”‚              â”‚                â”‚                     â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                      â”‚                                      â”‚
â”‚                      â–¼                                      â”‚
â”‚                  RÃ©ponse                                    â”‚
â”‚               (vÃ©rifiÃ©e !)                                  â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ce que l'agent peut faire :**

| ğŸ”§ CapacitÃ© | Outil | BÃ©nÃ©fice |
|:------------|:------|:---------|
| Lire des fichiers | `Read` | Comprendre le contexte rÃ©el |
| ExÃ©cuter du code | `Bash` | VÃ©rifier que Ã§a marche |
| Rechercher | `Grep`, `Glob` | Trouver l'information pertinente |
| Modifier | `Edit`, `Write` | Accomplir des tÃ¢ches |
| Tester | `npm test` | Valider les changements |

### 1.5.3 ğŸš€ La Synergie : Plus que la Somme des Parties

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸš€ SYNERGIE LLM + OUTILS                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚      ğŸ§  LLM          â”‚    â”‚      ğŸ”§ OUTILS       â”‚        â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”‚
â”‚  â”‚ âœ“ Comprend langage  â”‚    â”‚ âœ“ ExÃ©cute vraiment  â”‚        â”‚
â”‚  â”‚ âœ“ Connaissance      â”‚    â”‚ âœ“ Info actuelle     â”‚        â”‚
â”‚  â”‚ âœ“ Planification     â”‚    â”‚ âœ“ Actions rÃ©elles   â”‚        â”‚
â”‚  â”‚ âœ“ AdaptabilitÃ©      â”‚    â”‚ âœ“ PrÃ©cision 100%    â”‚        â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤        â”‚
â”‚  â”‚ âœ— Peut halluciner   â”‚    â”‚ âœ— Pas de jugement   â”‚        â”‚
â”‚  â”‚ âœ— Connaissance figÃ©eâ”‚    â”‚ âœ— Pas de crÃ©ativitÃ© â”‚        â”‚
â”‚  â”‚ âœ— Ne peut pas agir  â”‚    â”‚ âœ— Pas de contexte   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚             â”‚                          â”‚                    â”‚
â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                        â–¼                                    â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚            â”‚      ğŸ¤– AGENT        â”‚                         â”‚
â”‚            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                         â”‚
â”‚            â”‚ âœ“ Comprend ET agit  â”‚                         â”‚
â”‚            â”‚ âœ“ VÃ©rifie ses idÃ©es â”‚                         â”‚
â”‚            â”‚ âœ“ Info Ã  jour       â”‚                         â”‚
â”‚            â”‚ âœ“ CrÃ©atif ET prÃ©cis â”‚                         â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ 1.6 Exercices de ComprÃ©hension

Ces exercices ne sont pas des quiz â€” ce sont des **explorations** pour approfondir votre comprÃ©hension.

### ğŸ”¬ Exercice 1 : Tokenisation et Ses ConsÃ©quences

Prenez un extrait de code de votre projet (environ 50 lignes). Utilisez [tiktoken](https://github.com/openai/tiktoken) ou le [playground OpenAI](https://platform.openai.com/tokenizer) pour voir comment il est tokenisÃ©.

| Question | Exploration |
|:---------|:------------|
| 1ï¸âƒ£ | Combien de tokens ? Compare au nombre de mots |
| 2ï¸âƒ£ | Les noms de variables longs coÃ»tent-ils plus ? |
| 3ï¸âƒ£ | Commentaires FR vs EN â€” diffÃ©rence de coÃ»t ? |
| 4ï¸âƒ£ | Implications pour votre budget API ? |

### ğŸ¯ Exercice 2 : Provoquer une Hallucination

Essayez **dÃ©libÃ©rÃ©ment** de faire halluciner un LLM :

```
1. Demandez une fonction d'une bibliothÃ¨que inventÃ©e
2. Demandez de documenter un comportement faux
3. Demandez un exemple avec une API "future"
```

| Question | Exploration |
|:---------|:------------|
| 1ï¸âƒ£ | Le modÃ¨le admet-il son incertitude ? |
| 2ï¸âƒ£ | L'hallucination est-elle dÃ©tectable par un non-expert ? |
| 3ï¸âƒ£ | Comment un agent pourrait-il vÃ©rifier ? |

### ğŸ§  Exercice 3 : Les Limites du Raisonnement

Posez un problÃ¨me de logique **simple mais formulÃ© bizarrement** :

```
Au lieu de : "Si tous les A sont B, et X est un A, alors X est-il un B ?"

Essayez : "Si tous les zorblax sont des plimfos, et Grixel est un zorblax,
           Grixel est-il un plimfo ?"
```

| Question | Exploration |
|:---------|:------------|
| 1ï¸âƒ£ | La reformulation affecte-t-elle la performance ? |
| 2ï¸âƒ£ | Le modÃ¨le montre-t-il ses Ã©tapes de raisonnement ? |
| 3ï¸âƒ£ | Ces Ã©tapes sont-elles vraiment nÃ©cessaires ? |

---

## ğŸ¯ 1.7 Points ClÃ©s Ã  Retenir

### ğŸ“ Sur l'Architecture

| Concept | Point clÃ© |
|:--------|:----------|
| **Transformers** | Ont remplacÃ© la rÃ©currence par l'attention parallÃ¨le |
| **Attention** | Chaque token "regarde" tous les autres tokens |
| **Multi-tÃªtes** | Capturent diffÃ©rents types de relations simultanÃ©ment |
| **Profondeur** | Permet l'abstraction progressive (syntaxe â†’ sÃ©mantique â†’ intention) |

### âš™ï¸ Sur la GÃ©nÃ©ration

| Concept | Point clÃ© |
|:--------|:----------|
| **Next-token** | Les LLMs prÃ©disent le probable, pas le vrai |
| **TempÃ©rature** | ContrÃ´le le compromis prÃ©cision/crÃ©ativitÃ© |
| **Autoregression** | Pas de retour en arriÃ¨re â€” les erreurs se propagent |

### âš ï¸ Sur les Limites

| Limite | RÃ©alitÃ© |
|:-------|:--------|
| **Hallucinations** | IntrinsÃ¨ques, pas un bug â€” il faut les dÃ©tecter |
| **Contexte** | FenÃªtre limitÃ©e â€” il faut gÃ©rer la mÃ©moire |
| **Raisonnement** | Pattern matching, pas logique formelle |
| **Connaissance** | FigÃ©e Ã  la date d'entraÃ®nement |

### ğŸ¤– Sur les Agents

| Concept | Point clÃ© |
|:--------|:----------|
| **LLM seul** | Puissant mais ne peut pas agir |
| **Agent** | LLM + outils = capacitÃ© d'action |
| **ReAct** | Boucle Think â†’ Act â†’ Observe |
| **Synergie** | L'ensemble > somme des parties |

---

## ğŸŒ… Ã‰pilogue : La Fondation Est PosÃ©e

Marcus reposa son cafÃ©, maintenant froid.

â€” "Tu vois," dit-il, "un LLM n'est pas magique. C'est un systÃ¨me statistique extraordinairement sophistiquÃ©. Brillant pour reconnaÃ®tre des patterns. Mais il n'est pas omniscient, pas infaillible, et surtout â€” il ne peut rien faire par lui-mÃªme."

Lina hocha la tÃªte, les piÃ¨ces du puzzle s'assemblant.

â€” "Donc quand je veux construire un outil vraiment utile..."

â€” "Tu dois envelopper le LLM dans un systÃ¨me qui compense ses faiblesses. Des outils pour vÃ©rifier. De la mÃ©moire pour dÃ©passer le contexte. Des boucles de rÃ©troaction pour corriger les erreurs. C'est Ã§a, un **agent**."

Elle sourit, ouvrant un nouveau fichier dans son Ã©diteur.

â€” "Par oÃ¹ je commence ?"

â€” "Par comprendre ce qu'est vraiment un agent. Leurs types, leurs composants, leurs patterns. C'est le sujet du prochain chapitre."

---

| â¬…ï¸ PrÃ©cÃ©dent | ğŸ“– Sommaire | â¡ï¸ Suivant |
|:-------------|:-----------:|:-----------|
| [Avant-propos](00-avant-propos.md) | [Index](README.md) | [Le RÃ´le des Agents](02-role-des-agents.md) |
