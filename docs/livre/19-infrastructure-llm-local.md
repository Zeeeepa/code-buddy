# Chapitre 19 : Infrastructure LLM Local

> *"Pourquoi payer $0.03/1K tokens quand vous avez un GPU de gaming qui dort ?"*

---

## Ce Que Vous Allez Obtenir

| Module | Fonction | √âconomies |
|--------|----------|-----------|
| **GPU Monitor** | Surveillance VRAM en temps r√©el | √âvite les crashs OOM |
| **Ollama Embeddings** | Embeddings locaux (gratuits) | -100% co√ªt embeddings |
| **HNSW Vector Store** | Recherche vectorielle pure TypeScript | Pas de base externe |
| **Model Hub** | T√©l√©chargement mod√®les HuggingFace | Gestion simplifi√©e |
| **KV-Cache Config** | Optimisation m√©moire inf√©rence | +50% contexte |
| **Speculative Decoding** | Acc√©l√©ration g√©n√©ration | 2-3x plus rapide |
| **Benchmark Suite** | Mesure TTFT/TPS/p95 | Optimisation guid√©e |
| **Schema Validator** | Structured output fiable | Tool calling robuste |

**R√©sultat :** Un pipeline RAG enti√®rement local + inf√©rence optimis√©e.

---

## Pourquoi Aller Local ?

### Le Calcul Qui Fait Mal

```
Projet moyen :
‚îú‚îÄ‚îÄ 50,000 lignes de code
‚îú‚îÄ‚îÄ 1,000 chunks √† embedder
‚îú‚îÄ‚îÄ 768 dimensions
‚îî‚îÄ‚îÄ Rafra√Æchissement : 10x/jour

OpenAI ada-002 : 1,000 √ó 10 √ó 30 jours √ó $0.0001 = $3/mois
                 √ó 12 mois √ó 5 projets = $180/an

Avec Ollama : $0
```

### Quand Aller Local ?

| Sc√©nario | Cloud | Local | Verdict |
|----------|:-----:|:-----:|---------|
| Prototype rapide | ‚úÖ | ‚ùå | Cloud |
| Donn√©es sensibles | ‚ùå | ‚úÖ | **Local** |
| Volume √©lev√© | üí∏ | ‚úÖ | **Local** |
| Latence critique | ‚ùå | ‚úÖ | **Local** |
| GPU disponible | - | ‚úÖ | **Local** |

---

## 1. GPU Monitor : Surveiller Votre VRAM

### Le Probl√®me

```
$ ollama run devstral
Error: CUDA out of memory. Tried to allocate 2.00 GiB...
```

Charger un mod√®le 7B sans v√©rifier la VRAM disponible = crash garanti.

### La Solution

```typescript
import { GPUMonitor, initializeGPUMonitor } from './hardware/gpu-monitor.js';

// Initialisation (d√©tecte automatiquement NVIDIA/AMD/Apple/Intel)
const monitor = await initializeGPUMonitor();

// Statistiques VRAM
const stats = await monitor.getStats();
console.log(`VRAM: ${stats.usedVRAM}/${stats.totalVRAM} MB (${stats.usagePercent}%)`);

// Recommandation pour charger un mod√®le
const modelSizeMB = 4000; // Mod√®le 7B Q4
const recommendation = monitor.calculateOffloadRecommendation(modelSizeMB);

if (recommendation.shouldOffload) {
  console.log(`‚ö†Ô∏è ${recommendation.reason}`);
  console.log(`   Layers GPU sugg√©r√©s: ${recommendation.suggestedGpuLayers}`);
} else {
  console.log(`‚úÖ Assez de VRAM pour le mod√®le complet`);
}
```

### Architecture

![GPUMonitor Architecture](images/gpu-monitor-architecture.svg)

### D√©tection Multi-Vendor

```typescript
// Le monitor d√©tecte automatiquement votre GPU
switch (monitor.getVendor()) {
  case 'nvidia':  // nvidia-smi
  case 'amd':     // rocm-smi
  case 'apple':   // system_profiler SPDisplaysDataType
  case 'intel':   // intel_gpu_top
  case 'unknown': // Fallback m√©moire syst√®me
}
```

### Estimation VRAM par Mod√®le

| Mod√®le | Q4_K_M | Q5_K_M | Q8_0 |
|--------|--------|--------|------|
| 3B params | ~2.1 GB | ~2.5 GB | ~3.5 GB |
| 7B params | ~4.5 GB | ~5.3 GB | ~7.5 GB |
| 13B params | ~8.5 GB | ~10 GB | ~14 GB |
| 70B params | ~40 GB | ~48 GB | ~70 GB |

---

## 2. Ollama Embeddings : Embeddings Locaux

### Le Probl√®me

```typescript
// Chaque appel co√ªte de l'argent
const response = await openai.embeddings.create({
  model: 'text-embedding-ada-002',
  input: codeChunk,
});
```

### La Solution

```typescript
import {
  OllamaEmbeddingProvider,
  initializeOllamaEmbeddings
} from './context/codebase-rag/ollama-embeddings.js';

// Initialisation
const embedder = await initializeOllamaEmbeddings({
  baseUrl: 'http://localhost:11434',
  model: 'nomic-embed-text', // 768 dimensions, excellent pour code
});

// Embedding d'un chunk de code
const embedding = await embedder.embed(`
function calculateTotal(items: Item[]): number {
  return items.reduce((sum, item) => sum + item.price, 0);
}
`);

// Embedding batch (avec progression)
embedder.on('batch:progress', (current, total) => {
  console.log(`Embedding ${current}/${total}`);
});

const embeddings = await embedder.embedBatch([
  'const x = 1;',
  'function foo() {}',
  'class Bar extends Baz {}',
]);
```

### Mod√®les d'Embedding Recommand√©s

| Mod√®le | Dimensions | Sp√©cialit√© | Performance |
|--------|------------|------------|-------------|
| `nomic-embed-text` | 768 | Texte g√©n√©ral | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `mxbai-embed-large` | 1024 | Haute pr√©cision | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `all-minilm` | 384 | Vitesse | ‚≠ê‚≠ê‚≠ê |

### Installation Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# T√©l√©charger le mod√®le d'embedding
ollama pull nomic-embed-text

# V√©rifier
ollama list
```

### Calcul de Similarit√©

```typescript
// Similarit√© cosinus int√©gr√©e
const sim = embedder.similarity(embedding1, embedding2);
// sim ‚àà [-1, 1] o√π 1 = identique
```

---

## 3. HNSW Vector Store : Recherche Vectorielle Pure TypeScript

### Le Probl√®me

Les solutions existantes :
- **FAISS** : N√©cessite Python + bindings natifs
- **Pinecone/Weaviate** : Cloud, co√ªts, latence r√©seau
- **ChromaDB** : Serveur s√©par√© √† g√©rer

### La Solution

Un index HNSW (Hierarchical Navigable Small World) 100% TypeScript :

```typescript
import { HNSWVectorStore, getHNSWStore } from './context/codebase-rag/hnsw-store.js';

// Cr√©ation
const store = new HNSWVectorStore({
  dimensions: 768,        // Doit matcher votre embedder
  maxElements: 100000,    // Capacit√© max
  efConstruction: 200,    // Qualit√© construction (‚Üë = meilleur, plus lent)
  M: 16,                  // Connexions par noeud
});

// Ajout de vecteurs
store.add({
  id: 'src/utils/math.ts:calculateTotal',
  vector: embedding,
  metadata: {
    file: 'src/utils/math.ts',
    type: 'function',
    language: 'typescript',
  },
});

// Recherche k-NN
const results = store.search(queryEmbedding, 5);
// results = [{ id, distance, metadata }, ...]

// Persistance
await store.save('./cache/vectors.hnsw');
await store.load('./cache/vectors.hnsw');
```

### Comment Fonctionne HNSW

![HNSW Structure](images/hnsw-structure.svg)

**Complexit√©** : O(log N) au lieu de O(N) pour recherche lin√©aire

### Param√®tres de Tuning

| Param√®tre | Effet | Recommandation |
|-----------|-------|----------------|
| `M` | Connexions/noeud | 12-48 (16 par d√©faut) |
| `efConstruction` | Qualit√© build | 100-400 (200 par d√©faut) |
| `efSearch` | Qualit√© recherche | 50-200 (dynamique) |

```typescript
// Pour un petit index (< 10K vecteurs)
const smallStore = new HNSWVectorStore({
  dimensions: 768,
  M: 12,
  efConstruction: 100,
});

// Pour un gros index (> 100K vecteurs)
const largeStore = new HNSWVectorStore({
  dimensions: 768,
  M: 32,
  efConstruction: 400,
});
```

### Comparaison avec Alternatives

| Solution | D√©pendances | Latence | Complexit√© |
|----------|-------------|---------|------------|
| **HNSW (nous)** | 0 | ~1ms | TypeScript pur |
| FAISS | Python, bindings | ~0.5ms | Setup complexe |
| Pinecone | Cloud | ~50ms | Simple mais co√ªteux |
| ChromaDB | Serveur | ~10ms | Middleware |

---

## 4. Model Hub : Gestion des Mod√®les HuggingFace

### Le Probl√®me

```bash
# T√©l√©chargement manuel = douleur
wget https://huggingface.co/TheBloke/devstral-7B-GGUF/resolve/main/devstral-7b.Q4_K_M.gguf
# O√π le mettre ? Quelle quantization ? Combien de VRAM ?
```

### La Solution

```typescript
import {
  ModelHub,
  getModelHub,
  RECOMMENDED_MODELS,
  QUANTIZATION_TYPES
} from './models/model-hub.js';

// Initialisation
const hub = getModelHub({
  modelsDir: '~/.codebuddy/models',
});

// Lister les mod√®les disponibles
console.log(hub.formatModelList());
// ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
// ‚îÇ Mod√®le       ‚îÇ Taille ‚îÇ VRAM Q4  ‚îÇ Description           ‚îÇ
// ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
// ‚îÇ devstral-7b  ‚îÇ 7B     ‚îÇ ~4.5 GB  ‚îÇ Code, Mistral-based   ‚îÇ
// ‚îÇ codellama-7b ‚îÇ 7B     ‚îÇ ~4.5 GB  ‚îÇ Code, Meta            ‚îÇ
// ‚îÇ llama-3.2-3b ‚îÇ 3B     ‚îÇ ~2.1 GB  ‚îÇ G√©n√©ral, compact      ‚îÇ
// ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

// T√©l√©chargement avec progression
hub.on('download:progress', (percent, speed) => {
  console.log(`T√©l√©chargement: ${percent}% (${speed} MB/s)`);
});

await hub.download('devstral-7b', 'Q4_K_M');

// Estimer VRAM n√©cessaire
const model = RECOMMENDED_MODELS['devstral-7b'];
const vram = hub.estimateVRAM(model, 'Q4_K_M');
console.log(`VRAM estim√©e: ${vram} MB`);
```

### Mod√®les Recommand√©s pour le Code

| Mod√®le | Param√®tres | Forces | GPU Min |
|--------|------------|--------|---------|
| **devstral-7b** | 7B | Code g√©n√©ration, Mistral | 6 GB |
| **codellama-7b** | 7B | Code, Meta, multilingue | 6 GB |
| **llama-3.2-3b** | 3B | G√©n√©ral, rapide | 4 GB |
| **deepseek-coder-6.7b** | 6.7B | Code, instruction-tuned | 6 GB |

### Quantizations Expliqu√©es

![Guide des Quantizations GGUF](images/quantization-guide.svg)

---

## 5. Pipeline Complet : RAG Local

### Assemblage des Composants

```typescript
import { initializeGPUMonitor } from './hardware/gpu-monitor.js';
import { initializeOllamaEmbeddings } from './context/codebase-rag/ollama-embeddings.js';
import { HNSWVectorStore } from './context/codebase-rag/hnsw-store.js';

async function createLocalRAGPipeline() {
  // 1. V√©rifier les ressources GPU
  const gpu = await initializeGPUMonitor();
  const stats = await gpu.getStats();
  console.log(`GPU: ${stats.freeVRAM} MB disponibles`);

  // 2. Initialiser embeddings locaux
  const embedder = await initializeOllamaEmbeddings({
    model: 'nomic-embed-text',
  });

  // 3. Cr√©er le store vectoriel
  const store = new HNSWVectorStore({
    dimensions: embedder.getDimensions(),
    maxElements: 50000,
  });

  // 4. Indexer le code
  async function indexCodebase(files: string[]) {
    for (const file of files) {
      const content = await fs.readFile(file, 'utf-8');
      const chunks = chunkCode(content); // Votre chunking logic

      for (const chunk of chunks) {
        const embedding = await embedder.embed(chunk.text);
        store.add({
          id: `${file}:${chunk.start}-${chunk.end}`,
          vector: embedding,
          metadata: {
            file,
            type: chunk.type,
            code: chunk.text,
          },
        });
      }
    }

    // Sauvegarder l'index
    await store.save('./cache/codebase.hnsw');
  }

  // 5. Recherche s√©mantique
  async function search(query: string, k = 5) {
    const queryEmbedding = await embedder.embed(query);
    return store.search(queryEmbedding, k);
  }

  return { indexCodebase, search, store, embedder, gpu };
}
```

### Workflow Type

![RAG Pipeline Local](images/rag-pipeline-local.svg)

---

## 6. Configuration Recommand√©e

### Minimum Viable (Laptop)

```yaml
GPU: 4 GB VRAM (GTX 1650, M1)
Mod√®le: llama-3.2-3b Q4_K_M
Embeddings: all-minilm (384 dims)
HNSW: M=12, efConstruction=100
Index max: ~20,000 vecteurs
```

### Recommand√© (Desktop Gaming)

```yaml
GPU: 8-12 GB VRAM (RTX 3060/3070/4060)
Mod√®le: devstral-7b Q4_K_M
Embeddings: nomic-embed-text (768 dims)
HNSW: M=16, efConstruction=200
Index max: ~100,000 vecteurs
```

### Optimal (Workstation)

```yaml
GPU: 16+ GB VRAM (RTX 4080/4090, A4000)
Mod√®le: deepseek-coder-33b Q5_K_M
Embeddings: mxbai-embed-large (1024 dims)
HNSW: M=32, efConstruction=400
Index max: ~500,000 vecteurs
```

---

## 7. Troubleshooting

### Probl√®me : "CUDA out of memory"

```typescript
// Solution : R√©duire les layers GPU
const recommendation = gpu.calculateOffloadRecommendation(modelSizeMB);
// Utiliser recommendation.suggestedGpuLayers avec llama.cpp --n-gpu-layers
```

### Probl√®me : "Ollama connection refused"

```bash
# V√©rifier que Ollama tourne
ollama serve

# Ou via Docker
docker run -d -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
```

### Probl√®me : "Embeddings dimensions mismatch"

```typescript
// V√©rifier la coh√©rence
const embedderDims = embedder.getDimensions();
const storeDims = store.getDimensions();

if (embedderDims !== storeDims) {
  console.error(`Mismatch: embedder=${embedderDims}, store=${storeDims}`);
  // Recr√©er le store avec les bonnes dimensions
}
```

### Probl√®me : "HNSW search returns wrong results"

```typescript
// Augmenter efSearch pour plus de pr√©cision
store.setEfSearch(200); // Par d√©faut: 50

// Ou reconstruire avec efConstruction plus √©lev√©
const newStore = new HNSWVectorStore({
  dimensions: 768,
  efConstruction: 400, // Augment√©
});
```

---

## 8. KV-Cache Configuration : Optimiser la M√©moire d'Inf√©rence

### Le Probl√®me

Avec llama.cpp ou LM Studio, les valeurs par d√©faut sont souvent sous-optimales :

```bash
# Contexte limit√©, pas de quantification KV
llama-server --model qwen2.5-7b.gguf -c 4096
# Utilise ~2GB pour le KV-cache en FP16
```

### La Solution

```typescript
import {
  KVCacheManager,
  getKVCacheManager,
  MODEL_ARCHITECTURES
} from './inference/kv-cache-config.js';

// Initialisation avec d√©tection d'architecture
const kvManager = getKVCacheManager({
  contextLength: 16384,
  kvQuantization: 'q8_0',  // R√©duit m√©moire de 50%
  flashAttention: true,
});

// Configurer pour un mod√®le sp√©cifique
kvManager.setArchitecture('qwen2.5-7b-instruct');

// Estimation m√©moire
const estimate = kvManager.estimateMemory();
console.log(`KV-Cache: ${estimate.gpuMemoryMB} MB`);
console.log(`Fits in VRAM: ${estimate.fitsInVRAM ? '‚úÖ' : '‚ùå'}`);
console.log(`Recommendation: ${estimate.recommendation}`);

// G√©n√©rer les arguments llama.cpp
const args = kvManager.generateLlamaCppArgs();
// ['-c', '16384', '-b', '512', '--cache-type-k', 'q8_0', '--cache-type-v', 'q8_0', '-fa']
```

### Architectures Support√©es

| Mod√®le | Layers | Embed | Heads | KV-Heads | GQA |
|--------|--------|-------|-------|----------|-----|
| `qwen2.5-7b` | 28 | 3584 | 28 | 4 | ‚úÖ |
| `qwen2.5-14b` | 40 | 5120 | 40 | 8 | ‚úÖ |
| `llama-3.1-8b` | 32 | 4096 | 32 | 8 | ‚úÖ |
| `devstral-7b` | 32 | 4096 | 32 | 8 | ‚úÖ |
| `deepseek-coder-6.7b` | 32 | 4096 | 32 | 32 | MHA |

### Types de Quantification KV

![KV-Cache et Quantization](images/kv-cache-quantization.svg)

### Optimisation Automatique par VRAM

```typescript
// Optimisation automatique selon VRAM disponible
const optimized = kvManager.optimizeForVRAM(8000, 4000); // 8GB VRAM, 4GB mod√®le

// R√©sultat pour 4GB disponibles :
// {
//   contextLength: 8192,
//   kvQuantization: 'q8_0',
//   offloadMode: 'full_gpu',
//   flashAttention: true
// }
```

---

## 9. Speculative Decoding : Acc√©l√©rer la G√©n√©ration

### Le Probl√®me

La g√©n√©ration auto-r√©gressive est lente : 1 token = 1 forward pass.

```
Standard: Token1 ‚Üí Token2 ‚Üí Token3 ‚Üí Token4 ‚Üí Token5
          100ms    100ms    100ms    100ms    100ms = 500ms
```

### La Solution : Draft & Verify

```typescript
import {
  SpeculativeDecoder,
  getSpeculativeDecoder,
  RECOMMENDED_PAIRS
} from './inference/speculative-decoding.js';

// Cr√©er un d√©codeur avec mod√®le draft rapide
const decoder = getSpeculativeDecoder({
  draftModel: 'qwen2.5-0.5b',       // Petit mod√®le rapide
  targetModel: 'qwen2.5-7b',        // Grand mod√®le de v√©rification
  speculationLength: 4,             // Tokens √† sp√©culer
  acceptanceThreshold: 0.8,
});

// G√©n√©ration acc√©l√©r√©e
const result = await decoder.generate(
  'Explain quantum computing',
  async (prompt) => callDraftModel(prompt),
  async (prompt, tokens) => verifyWithTarget(prompt, tokens)
);

// Stats de performance
const stats = decoder.getStats();
console.log(`Acceptance rate: ${(stats.acceptanceRate * 100).toFixed(1)}%`);
console.log(`Speedup: ${stats.speedup.toFixed(2)}x`);
```

### Comment √áa Marche

![Speculative Decoding](images/speculative-decoding.svg)

### Paires Draft/Target Recommand√©es

| Target Model | Draft Model | Speedup Typique |
|-------------|-------------|-----------------|
| `qwen2.5-7b` | `qwen2.5-0.5b` | 2-3x |
| `qwen2.5-14b` | `qwen2.5-1.5b` | 2-2.5x |
| `llama-3.1-8b` | `llama-3.2-1b` | 2-3x |
| `devstral-7b` | `qwen2.5-0.5b` | 2-2.5x |

### Speculation Adaptative

```typescript
// Le d√©codeur ajuste automatiquement la longueur de sp√©culation
decoder.on('adaptiveAdjust', (event) => {
  console.log(`Adjusted speculation: ${event.oldLength} ‚Üí ${event.newLength}`);
  console.log(`Reason: ${event.reason}`);
});

// Si acceptanceRate < 50% ‚Üí r√©duit speculation
// Si acceptanceRate > 90% ‚Üí augmente speculation
```

---

## 10. Benchmark Suite : Mesurer les Performances

### Le Probl√®me

Sans m√©triques, impossible d'optimiser :

```
"Mon mod√®le semble rapide..." ‚Üí Non mesurable
"Mon mod√®le g√©n√®re 45 tok/s avec TTFT 180ms" ‚Üí Actionnable
```

### La Solution

```typescript
import {
  BenchmarkSuite,
  getBenchmarkSuite,
  DEFAULT_PROMPTS
} from './performance/benchmark-suite.js';

// Cr√©er une suite de benchmarks
const suite = getBenchmarkSuite({
  warmupRuns: 2,      // √âchauffement
  runs: 10,           // Mesures
  concurrency: 1,     // S√©quentiel ou parall√®le
  timeout: 60000,     // Timeout par run
});

// Callback pour mesurer votre mod√®le
const callback = async (prompt, onFirstToken) => {
  const response = await myLLM.generate(prompt, {
    onToken: (token, isFirst) => {
      if (isFirst && onFirstToken) onFirstToken();
    }
  });
  return {
    content: response.text,
    inputTokens: response.usage.input,
    outputTokens: response.usage.output,
  };
};

// Ex√©cuter les benchmarks
suite.on('run', (event) => {
  console.log(`Run ${event.runIndex}/${event.totalRuns}: ${event.latencyMs}ms`);
});

const results = await suite.run('qwen2.5-7b-Q4', callback);

// Afficher les r√©sultats format√©s
console.log(suite.formatResults(results));
```

### M√©triques Cl√©s

![Benchmark Results](images/benchmark-results.svg)

### Comparaison de Mod√®les

```typescript
// Benchmark du premier mod√®le
const results1 = await suite.run('qwen2.5-7b-Q4', callback1);

// Benchmark du second mod√®le
const results2 = await suite.run('qwen2.5-7b-Q8', callback2);

// Comparaison
const comparison = suite.compare(results1, results2);
console.log(`TTFT: ${comparison.ttft.improved ? '‚úÖ Improved' : '‚ùå Degraded'}`);
console.log(`  ${comparison.ttft.baseline}ms ‚Üí ${comparison.ttft.current}ms`);
console.log(`  ${comparison.ttft.percentChange > 0 ? '+' : ''}${comparison.ttft.percentChange.toFixed(1)}%`);
```

### Prompts de Test par Cat√©gorie

| Cat√©gorie | Prompt | Mesure |
|-----------|--------|--------|
| `simple` | "What is 2+2?" | Latence minimale |
| `code` | "Write a function to sort an array" | G√©n√©ration code |
| `reasoning` | "Explain quantum entanglement" | Raisonnement long |
| `creative` | "Write a haiku about programming" | Cr√©ativit√© |

---

## 11. Schema Validator : Structured Output Fiable

### Le Probl√®me

Les LLM g√©n√®rent du texte libre, mais vous avez besoin de JSON structur√© :

```typescript
// ‚ùå R√©ponse non structur√©e
"I would use the read_file tool with path /tmp/test.txt"

// ‚úÖ R√©ponse structur√©e
{ "tool": "read_file", "arguments": { "path": "/tmp/test.txt" } }
```

### La Solution

```typescript
import {
  SchemaValidator,
  getSchemaValidator,
  TOOL_CALL_SCHEMA,
  ACTION_PLAN_SCHEMA
} from './utils/schema-validator.js';

// Cr√©er un validateur avec coercion de types
const validator = getSchemaValidator({
  coerceTypes: true,      // "123" ‚Üí 123
  removeAdditional: true, // Supprimer propri√©t√©s inconnues
  useDefaults: true,      // Appliquer les valeurs par d√©faut
});

// D√©finir un schema personnalis√©
const schema = {
  type: 'object',
  properties: {
    action: { type: 'string', enum: ['read', 'write', 'delete'] },
    path: { type: 'string', minLength: 1 },
    content: { type: 'string' }
  },
  required: ['action', 'path']
};

// Valider une r√©ponse LLM
const llmResponse = `Here's what I'll do:
\`\`\`json
{"action": "read", "path": "/tmp/test.txt"}
\`\`\``;

const result = validator.validateResponse(llmResponse, schema);

if (result.valid) {
  console.log('Validated data:', result.data);
  // { action: 'read', path: '/tmp/test.txt' }
} else {
  console.log('Validation errors:', result.errors);
}
```

### Extraction JSON Intelligente

```typescript
// Le validateur extrait le JSON de n'importe quel format

// ‚úÖ JSON direct
validator.extractJSON('{"name": "test"}');

// ‚úÖ Code block markdown
validator.extractJSON('Here is the result:\n```json\n{"name": "test"}\n```');

// ‚úÖ JSON entour√© de texte
validator.extractJSON('The answer is {"name": "test"} as requested.');

// ‚úÖ Correction des trailing commas
validator.extractJSON('{"name": "test",}'); // ‚Üí {"name": "test"}
```

### Schemas Pr√©d√©finis

```typescript
import {
  TOOL_CALL_SCHEMA,     // Pour les appels d'outils
  ACTION_PLAN_SCHEMA,   // Pour les plans d'action
  CODE_EDIT_SCHEMA      // Pour les √©ditions de code
} from './utils/schema-validator.js';

// TOOL_CALL_SCHEMA
// {
//   tool: string (required),
//   arguments: object (required),
//   reasoning?: string
// }

// ACTION_PLAN_SCHEMA
// {
//   goal: string (required),
//   steps: [{ action: string, description: string }] (required),
//   estimatedSteps?: number
// }

// CODE_EDIT_SCHEMA
// {
//   file: string (required),
//   operation: 'create' | 'replace' | 'delete' (required),
//   oldContent?: string,
//   newContent?: string
// }
```

### Coercion de Types

| Input | Schema Type | Output |
|-------|-------------|--------|
| `"123"` | `number` | `123` |
| `"true"` | `boolean` | `true` |
| `1` | `boolean` | `true` |
| `123` | `string` | `"123"` |
| `["a", 1]` | `array<string>` | `["a", "1"]` |

### G√©n√©ration de Prompts

```typescript
// G√©n√©rer un prompt qui guide le LLM vers le bon format
const prompt = validator.createSchemaPrompt(schema);
// "Respond with valid JSON matching this schema:
//  {
//    action: string (one of: read, write, delete),
//    path: string (minimum 1 character),
//    content?: string
//  }
//  Required: action, path"
```

---

## Points Cl√©s

| Concept | √Ä Retenir |
|---------|-----------|
| **GPU Monitor** | Toujours v√©rifier VRAM avant de charger un mod√®le |
| **Ollama** | Embeddings gratuits, `nomic-embed-text` recommand√© |
| **HNSW** | O(log N), pas de d√©pendances, persistance JSON |
| **Quantization** | Q4_K_M = meilleur compromis qualit√©/taille |
| **Pipeline** | GPU Check ‚Üí Embed ‚Üí Store ‚Üí Search ‚Üí Generate |
| **KV-Cache** | Quantification q8_0 = -50% m√©moire, qualit√© pr√©serv√©e |
| **Speculative** | Draft + Target = 2-3x speedup g√©n√©ration |
| **Benchmark** | TTFT, TPS, p95 = m√©triques essentielles |
| **Schema** | Structured output = tool calling fiable |

---

| ‚Üê Pr√©c√©dent | Suivant ‚Üí |
|:-----------:|:---------:|
| [Ch.18 : Productivit√© CLI](18-productivite-cli.md) | [Annexe A : Transformers](annexe-a-transformers.md) |
