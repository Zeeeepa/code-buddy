# Chapitre 19 : Infrastructure LLM Local

> *"Pourquoi payer $0.03/1K tokens quand vous avez un GPU de gaming qui dort ?"*

---

## Ce Que Vous Allez Obtenir

| Module | Fonction | Ã‰conomies |
|--------|----------|-----------|
| **GPU Monitor** | Surveillance VRAM en temps rÃ©el | Ã‰vite les crashs OOM |
| **Ollama Embeddings** | Embeddings locaux (gratuits) | -100% coÃ»t embeddings |
| **HNSW Vector Store** | Recherche vectorielle pure TypeScript | Pas de base externe |
| **Model Hub** | TÃ©lÃ©chargement modÃ¨les HuggingFace | Gestion simplifiÃ©e |

**RÃ©sultat :** Un pipeline RAG entiÃ¨rement local, zÃ©ro dÃ©pendance cloud.

---

## Pourquoi Aller Local ?

### Le Calcul Qui Fait Mal

```
Projet moyen :
â”œâ”€â”€ 50,000 lignes de code
â”œâ”€â”€ 1,000 chunks Ã  embedder
â”œâ”€â”€ 768 dimensions
â””â”€â”€ RafraÃ®chissement : 10x/jour

OpenAI ada-002 : 1,000 Ã— 10 Ã— 30 jours Ã— $0.0001 = $3/mois
                 Ã— 12 mois Ã— 5 projets = $180/an

Avec Ollama : $0
```

### Quand Aller Local ?

| ScÃ©nario | Cloud | Local | Verdict |
|----------|:-----:|:-----:|---------|
| Prototype rapide | âœ… | âŒ | Cloud |
| DonnÃ©es sensibles | âŒ | âœ… | **Local** |
| Volume Ã©levÃ© | ğŸ’¸ | âœ… | **Local** |
| Latence critique | âŒ | âœ… | **Local** |
| GPU disponible | - | âœ… | **Local** |

---

## 1. GPU Monitor : Surveiller Votre VRAM

### Le ProblÃ¨me

```
$ ollama run devstral
Error: CUDA out of memory. Tried to allocate 2.00 GiB...
```

Charger un modÃ¨le 7B sans vÃ©rifier la VRAM disponible = crash garanti.

### La Solution

```typescript
import { GPUMonitor, initializeGPUMonitor } from './hardware/gpu-monitor.js';

// Initialisation (dÃ©tecte automatiquement NVIDIA/AMD/Apple/Intel)
const monitor = await initializeGPUMonitor();

// Statistiques VRAM
const stats = await monitor.getStats();
console.log(`VRAM: ${stats.usedVRAM}/${stats.totalVRAM} MB (${stats.usagePercent}%)`);

// Recommandation pour charger un modÃ¨le
const modelSizeMB = 4000; // ModÃ¨le 7B Q4
const recommendation = monitor.calculateOffloadRecommendation(modelSizeMB);

if (recommendation.shouldOffload) {
  console.log(`âš ï¸ ${recommendation.reason}`);
  console.log(`   Layers GPU suggÃ©rÃ©s: ${recommendation.suggestedGpuLayers}`);
} else {
  console.log(`âœ… Assez de VRAM pour le modÃ¨le complet`);
}
```

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GPUMonitor                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  initialize()          â”‚ DÃ©tection GPU (nvidia-smi, etc.)   â”‚
â”‚  getStats()            â”‚ VRAM totale/utilisÃ©e/libre         â”‚
â”‚  calculateOffloadRecommendation() â”‚ Layers GPU vs CPU      â”‚
â”‚  getRecommendedLayers()â”‚ Pour taille de modÃ¨le donnÃ©e       â”‚
â”‚  formatStats()         â”‚ Affichage formatÃ© pour CLI         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Events: stats, warning, critical                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### DÃ©tection Multi-Vendor

```typescript
// Le monitor dÃ©tecte automatiquement votre GPU
switch (monitor.getVendor()) {
  case 'nvidia':  // nvidia-smi
  case 'amd':     // rocm-smi
  case 'apple':   // system_profiler SPDisplaysDataType
  case 'intel':   // intel_gpu_top
  case 'unknown': // Fallback mÃ©moire systÃ¨me
}
```

### Estimation VRAM par ModÃ¨le

| ModÃ¨le | Q4_K_M | Q5_K_M | Q8_0 |
|--------|--------|--------|------|
| 3B params | ~2.1 GB | ~2.5 GB | ~3.5 GB |
| 7B params | ~4.5 GB | ~5.3 GB | ~7.5 GB |
| 13B params | ~8.5 GB | ~10 GB | ~14 GB |
| 70B params | ~40 GB | ~48 GB | ~70 GB |

---

## 2. Ollama Embeddings : Embeddings Locaux

### Le ProblÃ¨me

```typescript
// Chaque appel coÃ»te de l'argent
const response = await openai.embeddings.create({
  model: 'text-embedding-ada-002',
  input: codeChunk,
});
```

### La Solution

```typescript
import {
  OllamaEmbeddingProvider,
  initializeOllamaEmbeddings
} from './context/codebase-rag/ollama-embeddings.js';

// Initialisation
const embedder = await initializeOllamaEmbeddings({
  baseUrl: 'http://localhost:11434',
  model: 'nomic-embed-text', // 768 dimensions, excellent pour code
});

// Embedding d'un chunk de code
const embedding = await embedder.embed(`
function calculateTotal(items: Item[]): number {
  return items.reduce((sum, item) => sum + item.price, 0);
}
`);

// Embedding batch (avec progression)
embedder.on('batch:progress', (current, total) => {
  console.log(`Embedding ${current}/${total}`);
});

const embeddings = await embedder.embedBatch([
  'const x = 1;',
  'function foo() {}',
  'class Bar extends Baz {}',
]);
```

### ModÃ¨les d'Embedding RecommandÃ©s

| ModÃ¨le | Dimensions | SpÃ©cialitÃ© | Performance |
|--------|------------|------------|-------------|
| `nomic-embed-text` | 768 | Texte gÃ©nÃ©ral | â­â­â­â­ |
| `mxbai-embed-large` | 1024 | Haute prÃ©cision | â­â­â­â­â­ |
| `all-minilm` | 384 | Vitesse | â­â­â­ |

### Installation Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.ai/install.sh | sh

# TÃ©lÃ©charger le modÃ¨le d'embedding
ollama pull nomic-embed-text

# VÃ©rifier
ollama list
```

### Calcul de SimilaritÃ©

```typescript
// SimilaritÃ© cosinus intÃ©grÃ©e
const sim = embedder.similarity(embedding1, embedding2);
// sim âˆˆ [-1, 1] oÃ¹ 1 = identique
```

---

## 3. HNSW Vector Store : Recherche Vectorielle Pure TypeScript

### Le ProblÃ¨me

Les solutions existantes :
- **FAISS** : NÃ©cessite Python + bindings natifs
- **Pinecone/Weaviate** : Cloud, coÃ»ts, latence rÃ©seau
- **ChromaDB** : Serveur sÃ©parÃ© Ã  gÃ©rer

### La Solution

Un index HNSW (Hierarchical Navigable Small World) 100% TypeScript :

```typescript
import { HNSWVectorStore, getHNSWStore } from './context/codebase-rag/hnsw-store.js';

// CrÃ©ation
const store = new HNSWVectorStore({
  dimensions: 768,        // Doit matcher votre embedder
  maxElements: 100000,    // CapacitÃ© max
  efConstruction: 200,    // QualitÃ© construction (â†‘ = meilleur, plus lent)
  M: 16,                  // Connexions par noeud
});

// Ajout de vecteurs
store.add({
  id: 'src/utils/math.ts:calculateTotal',
  vector: embedding,
  metadata: {
    file: 'src/utils/math.ts',
    type: 'function',
    language: 'typescript',
  },
});

// Recherche k-NN
const results = store.search(queryEmbedding, 5);
// results = [{ id, distance, metadata }, ...]

// Persistance
await store.save('./cache/vectors.hnsw');
await store.load('./cache/vectors.hnsw');
```

### Comment Fonctionne HNSW

```
Niveau 3:  A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Z
           â”‚                      â”‚
Niveau 2:  A â”€â”€â”€ D â”€â”€â”€ M â”€â”€â”€ R â”€â”€ Z
           â”‚    â”‚     â”‚     â”‚    â”‚
Niveau 1:  Aâ”€Bâ”€Câ”€Dâ”€Eâ”€Fâ”€Mâ”€Nâ”€Oâ”€Râ”€Sâ”€Z
           â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
Niveau 0:  A B C D E F G H I ... Z (tous les vecteurs)

Recherche : Commencer en haut, descendre en suivant
            les voisins les plus proches
```

**ComplexitÃ©** : O(log N) au lieu de O(N) pour recherche linÃ©aire

### ParamÃ¨tres de Tuning

| ParamÃ¨tre | Effet | Recommandation |
|-----------|-------|----------------|
| `M` | Connexions/noeud | 12-48 (16 par dÃ©faut) |
| `efConstruction` | QualitÃ© build | 100-400 (200 par dÃ©faut) |
| `efSearch` | QualitÃ© recherche | 50-200 (dynamique) |

```typescript
// Pour un petit index (< 10K vecteurs)
const smallStore = new HNSWVectorStore({
  dimensions: 768,
  M: 12,
  efConstruction: 100,
});

// Pour un gros index (> 100K vecteurs)
const largeStore = new HNSWVectorStore({
  dimensions: 768,
  M: 32,
  efConstruction: 400,
});
```

### Comparaison avec Alternatives

| Solution | DÃ©pendances | Latence | ComplexitÃ© |
|----------|-------------|---------|------------|
| **HNSW (nous)** | 0 | ~1ms | TypeScript pur |
| FAISS | Python, bindings | ~0.5ms | Setup complexe |
| Pinecone | Cloud | ~50ms | Simple mais coÃ»teux |
| ChromaDB | Serveur | ~10ms | Middleware |

---

## 4. Model Hub : Gestion des ModÃ¨les HuggingFace

### Le ProblÃ¨me

```bash
# TÃ©lÃ©chargement manuel = douleur
wget https://huggingface.co/TheBloke/devstral-7B-GGUF/resolve/main/devstral-7b.Q4_K_M.gguf
# OÃ¹ le mettre ? Quelle quantization ? Combien de VRAM ?
```

### La Solution

```typescript
import {
  ModelHub,
  getModelHub,
  RECOMMENDED_MODELS,
  QUANTIZATION_TYPES
} from './models/model-hub.js';

// Initialisation
const hub = getModelHub({
  modelsDir: '~/.codebuddy/models',
});

// Lister les modÃ¨les disponibles
console.log(hub.formatModelList());
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚ ModÃ¨le       â”‚ Taille â”‚ VRAM Q4  â”‚ Description           â”‚
// â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
// â”‚ devstral-7b  â”‚ 7B     â”‚ ~4.5 GB  â”‚ Code, Mistral-based   â”‚
// â”‚ codellama-7b â”‚ 7B     â”‚ ~4.5 GB  â”‚ Code, Meta            â”‚
// â”‚ llama-3.2-3b â”‚ 3B     â”‚ ~2.1 GB  â”‚ GÃ©nÃ©ral, compact      â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

// TÃ©lÃ©chargement avec progression
hub.on('download:progress', (percent, speed) => {
  console.log(`TÃ©lÃ©chargement: ${percent}% (${speed} MB/s)`);
});

await hub.download('devstral-7b', 'Q4_K_M');

// Estimer VRAM nÃ©cessaire
const model = RECOMMENDED_MODELS['devstral-7b'];
const vram = hub.estimateVRAM(model, 'Q4_K_M');
console.log(`VRAM estimÃ©e: ${vram} MB`);
```

### ModÃ¨les RecommandÃ©s pour le Code

| ModÃ¨le | ParamÃ¨tres | Forces | GPU Min |
|--------|------------|--------|---------|
| **devstral-7b** | 7B | Code gÃ©nÃ©ration, Mistral | 6 GB |
| **codellama-7b** | 7B | Code, Meta, multilingue | 6 GB |
| **llama-3.2-3b** | 3B | GÃ©nÃ©ral, rapide | 4 GB |
| **deepseek-coder-6.7b** | 6.7B | Code, instruction-tuned | 6 GB |

### Quantizations ExpliquÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUANTIZATION GUIDE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Type     â”‚ Bits/Poids â”‚ QualitÃ©  â”‚ Usage                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Q4_K_M   â”‚ 4.5        â”‚ â­â­â­â­   â”‚ Meilleur rapport qualitÃ© â”‚
â”‚ Q5_K_M   â”‚ 5.5        â”‚ â­â­â­â­â­  â”‚ Haute qualitÃ©, +VRAM     â”‚
â”‚ Q6_K     â”‚ 6.5        â”‚ â­â­â­â­â­  â”‚ PrÃ¨s de FP16             â”‚
â”‚ Q8_0     â”‚ 8.0        â”‚ â­â­â­â­â­  â”‚ Quasi-lossless           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Recommandation :
â”œâ”€â”€ GPU 4-6 GB  â†’ Q4_K_M
â”œâ”€â”€ GPU 8-12 GB â†’ Q5_K_M ou Q6_K
â””â”€â”€ GPU 16+ GB  â†’ Q8_0 ou mÃªme FP16
```

---

## 5. Pipeline Complet : RAG Local

### Assemblage des Composants

```typescript
import { initializeGPUMonitor } from './hardware/gpu-monitor.js';
import { initializeOllamaEmbeddings } from './context/codebase-rag/ollama-embeddings.js';
import { HNSWVectorStore } from './context/codebase-rag/hnsw-store.js';

async function createLocalRAGPipeline() {
  // 1. VÃ©rifier les ressources GPU
  const gpu = await initializeGPUMonitor();
  const stats = await gpu.getStats();
  console.log(`GPU: ${stats.freeVRAM} MB disponibles`);

  // 2. Initialiser embeddings locaux
  const embedder = await initializeOllamaEmbeddings({
    model: 'nomic-embed-text',
  });

  // 3. CrÃ©er le store vectoriel
  const store = new HNSWVectorStore({
    dimensions: embedder.getDimensions(),
    maxElements: 50000,
  });

  // 4. Indexer le code
  async function indexCodebase(files: string[]) {
    for (const file of files) {
      const content = await fs.readFile(file, 'utf-8');
      const chunks = chunkCode(content); // Votre chunking logic

      for (const chunk of chunks) {
        const embedding = await embedder.embed(chunk.text);
        store.add({
          id: `${file}:${chunk.start}-${chunk.end}`,
          vector: embedding,
          metadata: {
            file,
            type: chunk.type,
            code: chunk.text,
          },
        });
      }
    }

    // Sauvegarder l'index
    await store.save('./cache/codebase.hnsw');
  }

  // 5. Recherche sÃ©mantique
  async function search(query: string, k = 5) {
    const queryEmbedding = await embedder.embed(query);
    return store.search(queryEmbedding, k);
  }

  return { indexCodebase, search, store, embedder, gpu };
}
```

### Workflow Type

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RAG Pipeline Local                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  1. CHECK GPU        2. EMBED           3. STORE             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚GPUMonitorâ”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ Ollama   â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  HNSW    â”‚         â”‚
â”‚  â”‚ VRAM: OK â”‚       â”‚Embeddingsâ”‚       â”‚VectorDB  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                              â”‚               â”‚
â”‚  4. QUERY           5. RETRIEVE         6. GENERATE         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  User    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚ Top-K    â”‚â”€â”€â”€â”€â”€â”€â–¶â”‚  LLM     â”‚         â”‚
â”‚  â”‚  Query   â”‚       â”‚ Results  â”‚       â”‚ (Local)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Configuration RecommandÃ©e

### Minimum Viable (Laptop)

```yaml
GPU: 4 GB VRAM (GTX 1650, M1)
ModÃ¨le: llama-3.2-3b Q4_K_M
Embeddings: all-minilm (384 dims)
HNSW: M=12, efConstruction=100
Index max: ~20,000 vecteurs
```

### RecommandÃ© (Desktop Gaming)

```yaml
GPU: 8-12 GB VRAM (RTX 3060/3070/4060)
ModÃ¨le: devstral-7b Q4_K_M
Embeddings: nomic-embed-text (768 dims)
HNSW: M=16, efConstruction=200
Index max: ~100,000 vecteurs
```

### Optimal (Workstation)

```yaml
GPU: 16+ GB VRAM (RTX 4080/4090, A4000)
ModÃ¨le: deepseek-coder-33b Q5_K_M
Embeddings: mxbai-embed-large (1024 dims)
HNSW: M=32, efConstruction=400
Index max: ~500,000 vecteurs
```

---

## 7. Troubleshooting

### ProblÃ¨me : "CUDA out of memory"

```typescript
// Solution : RÃ©duire les layers GPU
const recommendation = gpu.calculateOffloadRecommendation(modelSizeMB);
// Utiliser recommendation.suggestedGpuLayers avec llama.cpp --n-gpu-layers
```

### ProblÃ¨me : "Ollama connection refused"

```bash
# VÃ©rifier que Ollama tourne
ollama serve

# Ou via Docker
docker run -d -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
```

### ProblÃ¨me : "Embeddings dimensions mismatch"

```typescript
// VÃ©rifier la cohÃ©rence
const embedderDims = embedder.getDimensions();
const storeDims = store.getDimensions();

if (embedderDims !== storeDims) {
  console.error(`Mismatch: embedder=${embedderDims}, store=${storeDims}`);
  // RecrÃ©er le store avec les bonnes dimensions
}
```

### ProblÃ¨me : "HNSW search returns wrong results"

```typescript
// Augmenter efSearch pour plus de prÃ©cision
store.setEfSearch(200); // Par dÃ©faut: 50

// Ou reconstruire avec efConstruction plus Ã©levÃ©
const newStore = new HNSWVectorStore({
  dimensions: 768,
  efConstruction: 400, // AugmentÃ©
});
```

---

## Points ClÃ©s

| Concept | Ã€ Retenir |
|---------|-----------|
| **GPU Monitor** | Toujours vÃ©rifier VRAM avant de charger un modÃ¨le |
| **Ollama** | Embeddings gratuits, `nomic-embed-text` recommandÃ© |
| **HNSW** | O(log N), pas de dÃ©pendances, persistance JSON |
| **Quantization** | Q4_K_M = meilleur compromis qualitÃ©/taille |
| **Pipeline** | GPU Check â†’ Embed â†’ Store â†’ Search â†’ Generate |

---

| â† PrÃ©cÃ©dent | Suivant â†’ |
|:-----------:|:---------:|
| [Ch.18 : ProductivitÃ© CLI](18-productivite-cli.md) | [Annexe A : Transformers](annexe-a-transformers.md) |
